version: '3.8'

services:
  # ================= CORE SERVICES (Всегда включены) =================
  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_data:/qdrant/storage
    configs:
      - source: qdrant_config
        target: /qdrant/config/production.yaml
    networks:
      - ai-network

  neo4j:
    image: neo4j:5.26.9
    container_name: neo4j_lore
    restart: always
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc"]
    volumes:
      - ./neo4j_/data
    networks:
      - ai-network

  # ================= AI MODELS (Профиль: self-hosted-ai) =================
  # Запускаются только с флагом --profile self-hosted-ai
  llm-service:
    profiles: ["self-hosted-ai"]
    build:
      context: ./llama
      dockerfile: Dockerfile
    container_name: llama-llm
    ports:
      - "8000:8000"
    volumes:
      - models_volume:/models
    environment:
      - MODEL_PATH=/models/glm-4-flash.gguf
      - MODEL_URL=https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/resolve/main/GLM-4.7-Flash-Q2_K_L.gguf
      - GPU_LAYERS=${GPU_LAYERS:-99}
    command: >
      --port 8000
      --alias "unsloth/GLM-4.7-Flash"
      --fit on
      --temp 1.0
      --top-p 0.95
      --min-p 0.01
      --jinja
      --kv-unified
      --cache-type-k q8_0 --cache-type-v q8_0
      --flash-attn on
      --batch-size 4096 --ubatch-size 1024
      --ctx-size 13072
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - ai-network
    restart: unless-stopped

  embedder-service:
    profiles: ["self-hosted-ai"]
    build:
      context: ./llama
      dockerfile: Dockerfile
    container_name: llama-embedder
    ports:
      - "8080:8080"
    volumes:
      - models_volume:/models
    environment:
      - MODEL_PATH=/models/nomic-embed.gguf
      - MODEL_URL=https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q8_0.gguf
      - GPU_LAYERS=${GPU_LAYERS:-99}
    command: >
      --port 8080
      --embeddings
      -c 2048
    networks:
      - ai-network
    restart: unless-stopped

  # ================= APP SERVICE (Профиль: app) =================
  # Запускается только с флагом --profile app
  app:
    profiles: ["app"]
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: ai-app
    ports:
      - "3000:8080"
    environment:
      # Логика переключения режимов
      - SELF_HOSTED_LLM=${SELF_HOSTED_LLM:-false}
      - SELF_HOSTED_EMBEDDER=${SELF_HOSTED_EMBEDDER:-false}
      
      # Если self-hosted = true, используем внутренние адреса
      - LLM_BASE_URL=http://llm-service:8000
      - EMBEDDER_BASE_URL=http://embedder-service:8080
      
      # Если self-hosted = false, используем внешние (передаются из .env)
      - EXTERNAL_LLM_URL=${EXTERNAL_LLM_URL:-}
      - EXTERNAL_EMBEDDER_URL=${EXTERNAL_EMBEDDER_URL:-}
      - API_TOKEN=${API_TOKEN:-}
      
      # Базы данных
      - QDRANT_URL=http://qdrant:6333
      - NEO4J_URI=neo4j://neo4j_lore:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
    depends_on:
      - qdrant
      - neo4j
    networks:
      - ai-network
    restart: unless-stopped

volumes:
  models_volume:
    # Опционально: привязка к хосту для сохранения моделей
    driver_opts:
      type: none
      device: /models
      o: bind

configs:
  qdrant_config:
    content: |
      log_level: INFO

networks:
  ai-network:
    driver: bridge
