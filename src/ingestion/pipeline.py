import uuid
import logging
import numpy as np
from typing import Dict, List, Optional, Tuple
from llama_index.core import SimpleDirectoryReader, Document
from llama_index.core.text_splitter import SentenceSplitter
from llama_index.llms.openai_like import OpenAILike
from llama_index.core import PromptTemplate
#from llama_index.core.program import LLMTextCompletionProgram
from llama_index.embeddings.openai_like import OpenAILikeEmbedding
from llama_index.core.schema import MetadataMode
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from src.infrastructure.llama_adapter import SmartLlamaLLM
from src.infrastructure.smart_client import SmartOpenAI
from transformers import AutoTokenizer
from qdrant_client import models

from src.config import PipelineOptions, config
from src.ingestion.scene_splitter import AdaptiveMicroSplitter
from src.ingestion.mappers import RelationshipSanitizer
from src.ingestion.game_math import GameMath
from src.ingestion.synthesizer import EntitySynthesizer
from src.ingestion.classifier import HybridClassifier
from src.custom_program import LocalStructuredProgram as LLMTextCompletionProgram
from src.ingestion.graph_schemas import MoleculeType
from src.ingestion.semantic_projector import SemanticProjector
from src.ingestion.graph_builder import GraphBuilder
from src.ingestion.resolver import EntityResolver
from src.ingestion.schemas import ExtractedRelationship, ExtractionBatch
from src.registries.all_registries import (ATOMS, EVENTS, TOPOLOGIES, VERBS, ROLES, ARCS)

class IngestionEngine:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω ETL –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ data-as-code –¥–≤–∏–∂–∫–∞

    """
    def __init__(self, options: PipelineOptions = PipelineOptions()):
        self.options = options
        
        # 1. AI Models
        # self.llm = OpenAILike(
        #     model=config.llm.model_name,
        #     api_key=config.llm.api_key,
        #     api_base=config.llm.base_url,
        #     temperature=0.1
        # )
        # 1. –Ø–î–†–û: Smart Client (–¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∏ Structured Outputs)
        self.smart_client = SmartOpenAI(
            api_key=config.llm.api_key,
            base_url=config.llm.base_url,
            cache_dir="cache/global_llm"
        )
        
        # 2. –û–ë–ï–†–¢–ö–ê: LlamaIndex LLM (–¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤)
        self.llm = SmartLlamaLLM(
            model_name=config.llm.model_name,
            smart_client=self.smart_client
        )
        
        self.embedder = OpenAILikeEmbedding(
            model_name=config.vector.model_name,
            api_base=config.vector.base_url,
            api_key=config.vector.api_key
        )

        self.classifier = HybridClassifier(self.llm)
        self.synthesizer = EntitySynthesizer(self.llm)
        self.projector = SemanticProjector(self.embedder)
        
        # 2. Infrastructure
        self.qdrant = QdrantClient(url=config.qdrant.url)
        self._init_qdrant_collections()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        # GraphBuilder —Ç–µ–ø–µ—Ä—å —Å–∞–º –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è –∏–º–µ–µ—Ç Neo4jConnector –∏ –ª–æ–≥–∏–∫—É Loop
        self.graph_builder = GraphBuilder(synthesizer=self.synthesizer) 
        #self.resolver = EntityResolver(self.qdrant, self.embedder, self.llm)

        # 4. Tokenizer & Splitter (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å import)
        # –£–∫–∞–∂–∏ "gpt-4" –∏–ª–∏ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it").encode 
        
        # 5. Micro-Pass Program
        prompt_templ = PromptTemplate(
            "Analyze the text chunk as a Game Engine Parser.\n"
            "Extract distinct entities and system interactions based on the following Ontology:\n\n"
            
            "1. MOLECULES (Entities):\n"
            "- AGENT: Beings with Will (Characters, Monsters).\n"
            "- GROUP: Factions or Squads.\n"
            "- ASSET: Objects. Distinguish between 'ARTIFACT' (Unique/Named) and 'COMMODITY' (Gold, Food, Ammo).\n"
            "- CONSTRUCT: Spells, Skills, or Phenomena (e.g. 'Fireball', 'Curse'). NOT verbs.\n"
            "- LORE: Information, secrets, codes.\n\n"
            "   - **CRITICAL RULE**: The 'Description' MUST be a DIEGETIC FACT derived from THIS text chunk.\n"
            "     - GOOD: 'Holding a golden key', 'Falling slowly', 'Argues with the Queen'.\n"
            "     - BAD: 'Main character', 'A being with will', 'Entity in the story'.\n"
            "   - If an entity is present but does nothing significant, SKIP IT.\n\n"
            
            "2. INTERACTIONS (Verbs & Actions):\n"
            "- EXTRACT significant actions.\n"
            "- IF it involves skill checks/combat/resources -> Label as MECHANIC.\n"
            "- IF it is purely narrative (movement, atmosphere, emotion) -> Label as FLAVOR.\n"
            "- Example Mechanic: 'Attack', 'Cast Spell', 'Pick Lock'.\n"
            "- Example Flavor: 'The beast's eyes flamed', 'Alice stood in thought'.\n\n"
            
            "3. RELATIONSHIPS:\n"
            "- Connect entities logically (e.g. 'Alice' POSSESSES 'Key').\n\n"
            "- **RELATIONSHIP TYPES (Strict Taxonomy)**:\n"
            "   -- PHYSICAL: 'LOCATED_AT' (entity is inside place), 'POSSESSES' (holding item).\n"
            "   -- SOCIAL: 'KNOWS', 'LOVES', 'HATES', 'SERVES', 'COMMANDS'.\n"
            "   -- MENTAL: 'RECALLS' (memories), 'THINKS_OF' (thoughts), 'IMAGINES'.\n"
            "   -- LOGICAL: 'PART_OF' (finger part of hand), 'CAUSED' (event caused event).\n"
            
            "- RULES:\n"
            "   -- If someone THINKS about a place, use MENTAL type (NOT 'LOCATED_AT').\n"
            "   -- If 'Alice is in the King's presence', use SOCIAL ('NEAR' or 'SERVES'), NOT PHYSICAL 'LOCATED_AT'.\n\n"
            
            "TEXT CHUNK:\n{text_chunk}\n\n"
        )
        self.extractor_program = LLMTextCompletionProgram(
            output_cls=ExtractionBatch,
            llm=self.llm,
            prompt=prompt_templ, 
            verbose=True,
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )

        # 4. === RUNTIME CACHE ===
        # –ö—ç—à –¥–ª—è –≥–ª–∞–≥–æ–ª–æ–≤: "VerbName|System" -> "PrimitiveID"
        # –ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –≥–æ–Ω—è—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ "Attack"
        self._verb_cache: Dict[str, Optional[str]] = {}

    def reset_context(self):
        """
        –°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (Synthesizer, Cache) –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–æ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
        """
        print("üßπ Resetting Engine Context for new source...")
        
        # 1. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ö—Ä–∞–Ω—è—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ –ø–∞–º—è—Ç–∏
        # (EntitySynthesizer –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç _dossiers, GraphBuilder –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç —Å–≤—è–∑–∏)
        self.synthesizer = EntitySynthesizer(self.llm)
        self.graph_builder = GraphBuilder(synthesizer=self.synthesizer)
        
        # 2. –û—á–∏—â–∞–µ–º –∫—ç—à–∏
        self._verb_cache = {}
        
        # 3. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ú–æ–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å Resolver, –µ—Å–ª–∏ –æ–Ω –∫—ç—à–∏—Ä—É–µ—Ç —á—Ç–æ-—Ç–æ
        #self.resolver = EntityResolver(self.qdrant, self.embedder, self.llm)

    def _init_qdrant_collections(self):
        v_size = config.v_size 
        
        # 1. STATIC ONTOLOGY (–†–µ–µ—Å—Ç—Ä—ã)
        # –í—Å–µ —á–µ—Ä—Ç–µ–∂–∏ (–†–æ–ª–∏, –ì–ª–∞–≥–æ–ª—ã, –¢–æ–ø–æ–ª–æ–≥–∏–∏, –ê—Ä—Ö–µ—Ç–∏–ø—ã) –∂–∏–≤—É—Ç –∑–¥–µ—Å—å.
        if not self.qdrant.collection_exists("ontology_static"):
            self.qdrant.create_collection(
                collection_name="ontology_static",
                vectors_config=VectorParams(size=v_size, distance=Distance.COSINE),
                shard_number=1  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
            )
        
        # 2. DYNAMIC WORLD STATE (–ò–Ω—Å—Ç–∞–Ω—Å—ã)
        # –°—é–¥–∞ –ø–∏—à–µ—Ç _index_batch. –≠—Ç–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Ä–∞—Å—Ç—É—Ç –ø–æ –º–µ—Ä–µ –∏–≥—Ä—ã/—á—Ç–µ–Ω–∏—è.
        # molecules - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ NPC –∏ –ø—Ä–µ–¥–º–µ—Ç—ã
        # verbs - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ —Å—Ü–µ–Ω–µ (–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è!)
        # vibes - –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫—É—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞
        dynamic_collections = [
            "molecules", "verbs", "vibes", 
            "chronicle", "narrative_instances",
            "skeleton_locations"
        ]
        
        for name in dynamic_collections:
            if not self.qdrant.collection_exists(name):
                self.qdrant.create_collection(
                    collection_name=name,
                    vectors_config=VectorParams(size=v_size, distance=Distance.COSINE),
                    shard_number=1
                )

    def _index_roles(self, source_id: str):
        print("   üé≠ Indexing Roles...")
        points = []
        for role in ROLES.all():
            # –ú—è–≥–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é
            txt_vec = self._get_embedding(f"{role.id}. {role.description}")
            # –ñ–µ—Å—Ç–∫–∏–µ —Å—Ç–∞—Ç—ã –¥–ª—è –ª–æ–≥–∏–∫–∏
            stats = role.query_vector.model_dump()

            points.append(PointStruct(
                id=role.id, 
                vector=txt_vec,
                payload={
                    "doc_type": "role",     # <--- –§–∏–ª—å—Ç—Ä —Ç–∏–ø–∞
                    "source": source_id,    # <--- –ò—Å—Ç–æ—á–Ω–∏–∫
                    "description": role.description,
                    "required_tags": role.required_tags,
                    "stats": stats 
                }
            ))
        if points:
            self.qdrant.upsert("ontology_static", points)

    def _index_verbs(self, source_id: str):
        print("   ‚öîÔ∏è Indexing Verbs...")
        points = []
        for verb in VERBS.all():
            txt_vec = self._get_embedding(f"{verb.name} {verb.description}")
            stats = verb.vector.model_dump()
            
            points.append(PointStruct(
                id=verb.id,
                vector=txt_vec,
                payload={
                    "doc_type": "verb",
                    "source": source_id,
                    "name": verb.name,
                    "sphere": verb.sphere,
                    "stats": stats
                }
            ))
        if points:
            self.qdrant.upsert("ontology_static", points)

    def _index_topologies(self, source_id: str):
        print("   üó∫Ô∏è Indexing Topologies...")
        points = []
        for topo in TOPOLOGIES.all():
            txt_vec = self._get_embedding(f"{topo.name}. {topo.description}")
            stats = topo.query_vector.model_dump() # –í–Ω–∏–º–∞–Ω–∏–µ: –≤ –º–æ–¥–µ–ª–∏ –ø–æ–ª–µ query_vector
            
            points.append(PointStruct(
                id=topo.id,
                vector=txt_vec,
                payload={
                    "doc_type": "topology",
                    "source": source_id,
                    "name": topo.name,
                    "layout_type": topo.layout_type,
                    "stats": stats
                }
            ))
        if points:
            self.qdrant.upsert("ontology_static", points)

    def _index_event_archetypes(self, source_id: str):
        print("   üé¨ Indexing Event Archetypes...")
        points = []
        for evt in EVENTS.all(): 
            txt_vec = self._get_embedding(f"{evt.name}. {evt.description}")
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–∞–ª–∏—á–∏–µ vector –≤ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É –∏–ª–∏ SemanticVector()
            stats = getattr(evt, 'vector', None)
            stats_dump = stats.model_dump() if stats else {}
            
            points.append(PointStruct(
                id=evt.id,
                vector=txt_vec,
                payload={
                    "doc_type": "event_archetype",
                    "source": source_id,
                    "name": evt.name,
                    "consequences": evt.primary_consequence_tags,
                    "stats": stats_dump
                }
            ))
        if points:
            self.qdrant.upsert("ontology_static", points)

    def _index_arc_templates(self, source_id: str):
        print("   üìö Indexing Narrative Arc Templates...")
        points = []
        for template in ARCS.all():
            embedding = self._get_embedding(template.description)
            stats_dict = template.global_vector.model_dump() 
            
            points.append(PointStruct(
                id=template.id,
                vector=embedding,
                payload={
                    "doc_type": "arc_template",
                    "source": source_id,
                    "name": template.name,
                    "description": template.description,
                    "stats": stats_dict
                }
            ))
            
        if points:
            self.qdrant.upsert("ontology_static", points)

    def _get_embedding(self, text: str) -> List[float]:
        return self.embedder.get_text_embedding(text)
    
    def index_registries(self, source_id: str = "core"):
        """
        –ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤.
        Args:
            source_id: –º–µ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "core", "dlc_vampires", "mod_user123")
        """
        print(f"üìö Starting Registry Indexing (Source: {source_id})...")
        self._index_arc_templates(source_id)
        self._index_roles(source_id)
        self._index_verbs(source_id)
        self._index_topologies(source_id)
        self._index_event_archetypes(source_id)
        print("‚úÖ Registry Indexing Complete.")

    def process_directory(self, input_dir: str, source_id: str):
        """
        Args:
            input_dir: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–µ–∫—Å—Ç–æ–º
            source_id: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –º–∏—Ä–∞/–∫–Ω–∏–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'alice_wonderland', 'dune_1')
                       –≠—Ç–æ—Ç ID –±—É–¥–µ—Ç –∑–∞–ø–∏—Å–∞–Ω –≤ payload –∫–∞–∂–¥–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞.
        """
        reader = SimpleDirectoryReader(input_dir)
        documents = reader.load_data()
        
        print(f"üöÄ Processing Source '{source_id}' ({len(documents)} docs)...")

        for doc in documents:
            source_ref = doc.doc_id 
            print(f"\nüìÑ Processing Document: {source_ref}")

            # PHASE 1
            scene_ranges, entity_registry = self.graph_builder.build_world_skeleton(doc.text, source_ref)
            
            # PHASE 2: –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º source_id –≤–Ω—É—Ç—Ä—å
            self._process_micro_chunks(doc, source_ref, scene_ranges, entity_registry, source_id)

    def _process_micro_chunks(self, document: Document, source_ref: str, 
                              scene_ranges: List[tuple], 
                              entity_registry: Dict[str, str],
                              source_id: str
        ):
        
        print(f"   üîç Micro-pass (Adaptive Semantic with Cursor)...")
        
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –°–ø–ª–∏—Ç—Ç–µ—Ä–∞
        micro_parser = AdaptiveMicroSplitter(
            embedder=self.embedder,
            tokenizer=self.tokenizer, 
            min_tokens=500,
            max_tokens=2000,
            base_threshold=0.35
        )
        
        # 2. –ù–∞—Ä–µ–∑–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        nodes = micro_parser.get_nodes_from_documents([document])
        
        for i, node in enumerate(nodes):
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞
            node_text = node.get_content(metadata_mode=MetadataMode.NONE)
            
            # === –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ===
            # –ë–µ—Ä–µ–º —Ç–æ—á–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞
            start_idx = node.metadata.get("start_char_idx", 0)
            end_idx = node.metadata.get("end_char_idx", len(node_text))
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä —á–∞–Ω–∫–∞ (–∞–±—Å–æ–ª—é—Ç–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)
            chunk_center = start_idx + (end_idx - start_idx) // 2
            
            # 3. –ü–æ–∏—Å–∫ –ú–∞–∫—Ä–æ-–ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–°—Ü–µ–Ω–∞, –õ–æ–∫–∞—Ü–∏—è)
            # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–æ—á–Ω—ã–π —Ü–µ–Ω—Ç—Ä —á–∞–Ω–∫–∞
            loc_id, context_data = self._find_location_for_offset(chunk_center, scene_ranges)
            
            # 4. –ò–Ω—ä–µ–∫—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (Context Injection)
            # –ï—Å–ª–∏ —Å—Ü–µ–Ω–∞ –º–µ–Ω—Ç–∞–ª—å–Ω–∞—è, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
            context_prefix = ""
            if context_data:
                label = context_data.get('label', 'Unknown Context')
                sc_type = context_data.get('type', 'PHYSICAL')
                
                if sc_type != "PHYSICAL":
                    context_prefix = (
                        f"[SCENE TYPE: {sc_type} | CONTEXT: {label}]\n"
                        "NOTE: Entities here are likely MEMORIES or THOUGHTS. "
                        "Mark relationships as 'MENTAL' or 'REFERENCE' where appropriate.\n\n"
                    )
                else:
                    # –î–∞–∂–µ –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω –ø–æ–ª–µ–∑–Ω–æ –∑–Ω–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ ("Alice falls down")
                    context_prefix = f"[SCENE: {label}]\n"
            
            final_chunk_text = context_prefix + node_text
            
            # 5. –í—ã–∑–æ–≤ –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
            # –ü–µ—Ä–µ–¥–∞–µ–º current_tick –∫–∞–∫ –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —á–∞–Ω–∫–∞ (–∏–ª–∏ –º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤)
            current_tick = i 
            
            try:
                data: ExtractionBatch = self.extractor_program(text_chunk=final_chunk_text)
                
                # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è (–ø–µ—Ä–µ–¥–∞–µ–º —Ç–æ—á–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–π loc_id)
                self._index_batch(data, source_ref, loc_id, entity_registry, source_id, current_tick=current_tick)
                
            except Exception as e:
                logging.error(f"Error extracting from micro-chunk {i}: {e}")
    
    def _find_location_for_offset(self, offset: int, ranges: List[tuple]) -> Tuple[Optional[str], Optional[dict]]:
        """
        –ò—â–µ—Ç, –≤ –∫–∞–∫–æ–π —Å—Ü–µ–Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ç–æ—á–∫–∞ offset.
        ranges = [(0, 5000, 'uuid1'), (5000, 10000, 'uuid2')...]
        """
        for start, end, loc_uuid, context_data in ranges: # <--- 4 args
            if start <= offset < end:
                return loc_uuid, context_data # <--- –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç!
        return None, None

    def _index_batch(self, batch: ExtractionBatch, source_ref: str, 
                     loc_id: str, entity_registry: Dict[str, str], 
                     source_id: str,
                     current_tick: int = -1):
        
        points = {"molecules": [], "verbs": [], "vibes": []}

        # 1. MOLECULES (Accumulation Phase)
        for m in batch.molecules:
            clean_name = m.name.lower().strip()
            
            # –ê. [cite_start]ID RESOLUTION (Deterministic) [cite: 1]
            if clean_name in entity_registry:
                mol_id = entity_registry[clean_name]
                is_canonical = True
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–º–µ—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –æ—Ç –∏–º–µ–Ω–∏
                mol_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, clean_name))
                is_canonical = False

            # –ë. COLLECT OBSERVATION (–í–º–µ—Å—Ç–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞)
            # –ú—ã –ø—Ä–æ—Å—Ç–æ —Å–∫–ª–∞–¥—ã–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤ –ø–∞–ø–∫—É.
            self.synthesizer.collect(
                uid=mol_id,
                observation=m.description,
                metadata={
                    "name": m.name,
                    "category": m.category,
                    "subtype": m.subtype,
                    "source_doc": source_ref,
                    "world_id": source_id
                }
            )

            # –í. CREATE STUB NODE (–ó–∞–≥–ª—É—à–∫–∞ –≤ Neo4j)
            # –ù–∞–º –Ω—É–∂–µ–Ω —É–∑–µ–ª –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å, —á—Ç–æ–±—ã –ø—Ä–∏–≤—è–∑–∞—Ç—å –∫ –Ω–µ–º—É —Å–≤—è–∑–∏ (Links).
            # –°—Ç–∞—Ç—ã –ø–æ–∫–∞ –ø–æ –Ω—É–ª—è–º. –ú—ã –æ–±–Ω–æ–≤–∏–º –∏—Ö –≤ –ø–æ—Å—Ç-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–µ.
            self.graph_builder.neo4j.upsert_molecule(
                mol_id, m.name, m.category, 
                semantic_stats=None # –ü–æ–∫–∞ –ø—É—Å—Ç–æ
            )
            
            if loc_id:
                self.graph_builder.neo4j.link_molecule_to_location(mol_id, loc_id)

            # –ú—ã –ù–ï –ø–∏—à–µ–º –≤ Qdrant –∑–¥–µ—Å—å –º–æ–ª–µ–∫—É–ª—É —Ü–µ–ª–∏–∫–æ–º, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤–µ–∫—Ç–æ—Ä –±—É–¥–µ—Ç "–º—É—Å–æ—Ä–Ω—ã–º".
            # –ù–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ process_relationships, –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å —á–µ—Ä–Ω–æ–≤–∏–∫.
            # –î–∞–≤–∞–π—Ç–µ –∑–∞–ø–∏—à–µ–º —á–µ—Ä–Ω–æ–≤–∏–∫, —á—Ç–æ–±—ã Resolver —Ä–∞–±–æ—Ç–∞–ª.
            embedding = self._get_embedding(f"{m.name} {m.description}")
            points["molecules"].append(PointStruct(
                id=mol_id, vector=embedding, payload={
                    "name": m.name, "type": "molecule", "is_draft": True,
                    "source_id": source_id
                }
            ))

        # =========================================================================
        # 2. VERBS (System Mechanics)
        # =========================================================================
        for v in batch.verbs:
            # 1. –°–¢–û–ü-–°–õ–û–í–ê (–ú—É—Å–æ—Ä–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä)
            # –ï—Å–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ —Å–ª–∏—à–∫–æ–º –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ, —Å—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Flavor, –º–∏–Ω—É—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä.
            GARBAGE_VERBS = ["did", "do", "does", "be", "is", "was", "went", "go", "said", "look", "saw"]
            clean_name = v.name.lower().strip()
            
            is_garbage = clean_name in GARBAGE_VERBS
            # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ "did so carefully" ‚Äî —ç—Ç–æ –º—É—Å–æ—Ä –¥–ª—è –º–µ—Ö–∞–Ω–∏–∫–∏.
            if len(clean_name) < 3 or is_garbage:
                if loc_id:
                     # –ü—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –¥–æ—Å—å–µ —Å—Ü–µ–Ω—ã –∫–∞–∫ —Ç–µ–∫—Å—Ç
                     self.synthesizer.collect_scene_beat(loc_id, f"{v.name}: {v.context_usage}", tick=current_tick)
                continue

            # 2. CACHE & CLASSIFY
            cache_key = f"{v.name.lower()}|{v.implied_system}"
            primitive_id = None
            
            if cache_key in self._verb_cache:
                primitive_id = self._verb_cache[cache_key]
            else:
                # [cite_start]–ï—Å–ª–∏ LLM —Å–∞–º–∞ –ø–æ–º–µ—Ç–∏–ª–∞ —ç—Ç–æ –∫–∞–∫ FLAVOR, –¥–∞–∂–µ –Ω–µ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–∫–∞—Ç—å –º–µ—Ö–∞–Ω–∏–∫—É [cite: 5]
                if v.implied_system == "FLAVOR" or "narrative" in v.implied_system.lower():
                    primitive_id = None
                else:
                    # –°—Ç—Ä–æ–≥–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                    query_text = f"{v.name}. {v.context_usage}. System: {v.implied_system}"
                    if self.options.project_verbs:
                        primitive_id = self.classifier.classify(
                            query_text=query_text,
                            registry=VERBS,
                            threshold_high=0.88, # –û—á–µ–Ω—å —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –º–∞—Ç—á–∞
                            threshold_low=0.65,  # –ü–æ–¥–Ω—è–ª–∏ –Ω–∏–∂–Ω–∏–π –ø–æ—Ä–æ–≥ (–±—ã–ª–æ 0.55/0.60)
                            top_k=3
                        )
                self._verb_cache[cache_key] = primitive_id

            # 3. BRANCHING (Mechanic vs Flavor)
            if primitive_id:
                # === MECHANIC ===
                # –≠—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è –∏–≥—Ä–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞ (Attack, Cast)
                
                # –°—á–∏—Ç–∞–µ–º "–£–º–Ω—ã–µ –°—Ç–∞—Ç—ã" —á–µ—Ä–µ–∑ GameMath
                emb = self._get_embedding(f"{v.name} {v.force_desc}")
                raw_stats = self.projector.project(emb)
                # [cite_start]–ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É —Å–∏—Å—Ç–µ–º—ã (Combat/Magic) [cite: 3]
                final_stats = GameMath.calculate_action_stats(raw_stats, v.implied_system)

                # Upsert Verb Point
                verb_id = str(uuid.uuid4())
                points["verbs"].append(PointStruct(
                    id=verb_id, vector=emb, payload={
                        "name": v.name,
                        "system": v.implied_system,
                        "primitive_id": primitive_id,
                        "stats": final_stats, # –£–∂–µ –Ω–µ 0.5!
                        "location_id": loc_id,
                        "source_id": source_id
                    }
                ))
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å—Ü–µ–Ω—É —Å –ø–æ–º–µ—Ç–∫–æ–π [Mechanic]
                if loc_id:
                    self.synthesizer.collect_scene_beat(loc_id, f"[Mechanic] {v.name}", tick=current_tick)

            else:
                # === FLAVOR ===
                # "Alice danced", "Knave did so carefully"
                # –ú—ã –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç–æ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é 'verbs' –≤ Qdrant (—á—Ç–æ–±—ã –Ω–µ –º—É—Å–æ—Ä–∏—Ç—å –ø–æ–∏—Å–∫ –º–µ—Ö–∞–Ω–∏–∫).
                # –ú—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç–æ –¢–û–õ–¨–ö–û –≤ –¥–æ—Å—å–µ —Å—Ü–µ–Ω—ã –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.
                
                if loc_id:
                    self.synthesizer.collect_scene_beat(loc_id, f"{v.name}: {v.context_usage}", tick=current_tick)

        # =========================================================================
        # 3. VIBES (Atmosphere)
        # =========================================================================
        batch_vibe_stats = {"material": [], "vitality": [], "social": [], "cognitive": []}

        for vb in batch.vibes:
            # –§–∏–ª—å—Ç—Ä —Å–æ–≤—Å–µ–º –º—É—Å–æ—Ä–∞
            if len(vb.snippet) < 5: continue

            vibe_id = str(uuid.uuid4())
            emb = self._get_embedding(vb.snippet)
            raw_stats = self.projector.project(emb)
            
            # === –ü–†–ò–ú–ï–ù–Ø–ï–ú GAME MATH ===
            # –≠—Ç–æ —Ä–∞–∑–¥–≤–∏–Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è 0.5 -> 0.1/0.9 –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–≥–æ–≤
            final_stats = GameMath.calculate_vibe_stats(raw_stats, vb.tags)
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ª–æ–∫–∞—Ü–∏–∏
            for k, v in final_stats.items():
                if k in batch_vibe_stats:
                    batch_vibe_stats[k].append(v)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
            # –ï—Å–ª–∏ –≤–∞–π–± –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ FLAVOR/LORE, –æ–Ω –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è RAG, –Ω–æ –Ω–µ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∫–∏.
            points["vibes"].append(PointStruct(
                id=vibe_id, vector=emb, payload={
                    "snippet": vb.snippet,
                    "tags": vb.tags,
                    "stats": final_stats, # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—ã
                    "location_id": loc_id,
                    "source_id": source_id
                }
            ))

        # --- AGGREGATION ---
        if loc_id and any(batch_vibe_stats.values()):
            avg_stats = {}
            for axis, values in batch_vibe_stats.items():
                if values:
                    # –°—Ä–µ–¥–Ω–µ–µ –ø–æ —É–∂–µ "—Å–º–µ—â–µ–Ω–Ω—ã–º" –∑–Ω–∞—á–µ–Ω–∏—è–º –¥–∞—Å—Ç —Å–∏–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
                    avg_stats[axis] = sum(values) / len(values)
                else:
                    avg_stats[axis] = 0.0
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤ –≥—Ä–∞—Ñ–µ
            self.graph_builder.neo4j.update_location_atmosphere(loc_id, avg_stats, weight=0.3)
            
            print(f"   üé® Painted Atmosphere for Location {loc_id}: {avg_stats}")

        # =========================================================================
        # 4. FINAL UPSERT & RELATIONS
        # =========================================================================
        
        # Upsert –≤—Å–µ—Ö —Ç–æ—á–µ–∫
        for col_name, pts in points.items():
            if pts:
                self.qdrant.upsert(collection_name=col_name, points=pts)
        
        # --- RELATIONSHIPS PROCESSING ---

        mol_points = points["molecules"]
        if mol_points:
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ "–ò–º—è –∏–∑ —Ç–µ–∫—Å—Ç–∞" -> "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π UUID" –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
            local_name_map = {m.name: mol_points[i].id for i, m in enumerate(batch.molecules)}
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Ä–µ–µ—Å—Ç—Ä–æ–º
            full_name_map = {**entity_registry, **local_name_map}
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–≤—è–∑–∏
            self._process_relationships(batch.relationships, full_name_map, loc_id)


    def run_post_processing(self, source_id: str):
        print("\n‚öôÔ∏è Starting Post-Processing...")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞—Å–∫–∏ (BIAS)
        # –ó–Ω–∞—á–µ–Ω–∏—è > 1.0 —É—Å–∏–ª–∏–≤–∞—é—Ç —Å–∏–≥–Ω–∞–ª, < 1.0 –ø–æ–¥–∞–≤–ª—è—é—Ç —à—É–º.
        TYPE_BIAS = {
            "AGENT":     {"mat": 0.5, "vit": 1.2, "soc": 1.2, "cog": 1.2},
            "GROUP":     {"mat": 0.2, "vit": 0.8, "soc": 2.0, "cog": 1.0},
            "ASSET":     {"mat": 1.5, "vit": 0.5, "soc": 0.1, "cog": 0.3},
            "LOCATION":  {"mat": 1.2, "vit": 1.0, "soc": 0.5, "cog": 0.5},
            "CONSTRUCT": {"mat": 0.1, "vit": 0.5, "soc": 0.8, "cog": 1.5},
            "LORE":      {"mat": 0.0, "vit": 0.0, "soc": 0.5, "cog": 2.0},
            # Fallback
            "UNKNOWN":   {"mat": 1.0, "vit": 1.0, "soc": 1.0, "cog": 1.0}
        }

        # 1. CONSOLIDATION (–°–∫–ª–µ–π–∫–∞ –¥—É–±–ª–µ–π)
        self.synthesizer.consolidate_dossiers()
        self.synthesizer.consolidate_locations()

        # ---------------------------------------------------------------------
        # STEP 2: LOCATION BLUEPRINT (–°—Ç—Ä–æ–∏–º –º–∏—Ä –ø–µ—Ä–≤—ã–º, —á—Ç–æ–±—ã —Å—É—â–Ω–æ—Å—Ç—è–º –±—ã–ª–æ –≥–¥–µ –∂–∏—Ç—å)
        # ---------------------------------------------------------------------
        print("üè∞ Synthesizing Location Blueprints & Physics...")
        
        loc_uids = list(self.synthesizer._location_dossiers.keys())
        
        # –ú–∞—Å–∫–∞ –¥–ª—è —Ñ–∏–∑–∏–∫–∏ –ª–æ–∫–∞—Ü–∏–π (–õ–æ–∫–∞—Ü–∏–∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã)
        LOCATION_BIAS = {"mat": 1.2, "vit": 1.0, "soc": 0.5, "cog": 0.5}

        for loc_id in loc_uids:
            # 1. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–ª–µ–µ–Ω–Ω—ã–µ (–¥—É–±–ª–∏)
            if loc_id in self.synthesizer._redirect_map:
                continue

            # 2. –°–ò–ù–¢–ï–ó (Blueprint)
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤
            loc_data = self.synthesizer.synthesize_location(loc_id)
            if not loc_data:
                continue

            # 3. –§–ò–ó–ò–ö–ê (Physics Projection)
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –¢–û–õ–¨–ö–û —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é —á–∞—Å—Ç—å ("Stone walls, narrow corridor")
            # –ú—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤–∞–π–±—ã (—Å—Ç—Ä–∞—Ö, —Ç–µ–º–Ω–æ—Ç—É), —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —á–∏—Å—Ç—É—é —Ñ–∏–∑–∏–∫—É.
            physics_text = f"{loc_data.canonical_name}. {' '.join(loc_data.geometry_tags)} {' '.join(loc_data.material_tags)}. {loc_data.summary}"
            
            embedding = self._get_embedding(physics_text)
            raw_stats = self.projector.project(embedding)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º Bias
            final_physics = {
                k: v * LOCATION_BIAS.get(k[:3], 1.0) 
                for k, v in raw_stats.items()
            }

            # 4. UPSERT GRAPH (–û–±–Ω–æ–≤–ª—è–µ–º –°–∫–µ–ª–µ—Ç)
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç—ã –≤ val_*, –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Ç–µ–≥–∏
            self.graph_builder.neo4j.update_location_physics(
                loc_id, 
                name=loc_data.canonical_name,
                description=loc_data.summary,
                physics_stats=final_physics,
                geometry_tags=loc_data.geometry_tags
            )

            # 5. UPSERT QDRANT (Skeleton Collection)
            # –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è RAG: "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∫–∞–º–µ–Ω–Ω—ã–π –∑–∞–ª?"
            self.qdrant.upsert("skeleton_locations", [PointStruct(
                id=loc_id,
                vector=embedding,
                payload={
                    "name": loc_data.canonical_name,
                    "description": loc_data.summary,
                    "exits": loc_data.detected_exits,
                    "physics": final_physics,
                    "type": "location",
                    "importance": loc_data.importance_score
                }
            )])

        print("‚úÖ Locations processed.")

        # ---------------------------------------------------------------------
        # STEP 3: CLEANUP LOCATIONS
        # ---------------------------------------------------------------------
        print("üßπ Cleaning up merged locations...")
        for old_id, new_id in self.synthesizer._redirect_map.items():
            # –ï—Å–ª–∏ —ç—Ç–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç –ª–æ–∫–∞—Ü–∏–∏ (–ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ dossiers)
            if old_id in self.synthesizer._location_dossiers:
                # –í Neo4j –Ω–∞–¥–æ –±—ã –ø–µ—Ä–µ–∫–∏–Ω—É—Ç—å —Å–≤—è–∑–∏ CONNECTED_TO, –Ω–æ —ç—Ç–æ —Å–ª–æ–∂–Ω–æ –±–µ–∑ APOC.
                # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è: —É–¥–∞–ª—è–µ–º –¥—É–±–ª—å. –û—Å–Ω–æ–≤–Ω–æ–π —É–∑–µ–ª –æ—Å—Ç–∞–ª—Å—è.
                # (–í –∏–¥–µ–∞–ª–µ Pass 1 —Å—Ç—Ä–æ–∏—Ç —Å–≤—è–∑–∏ –ø–æ –∏–º–µ–Ω–∞–º, —Ç–∞–∫ —á—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π —É–∑–µ–ª —É–∂–µ –∏–º–µ–µ—Ç —Å–≤—è–∑–∏)
                self.graph_builder.neo4j.delete_location(old_id)
                self.qdrant.delete("skeleton_locations", points_selector=[old_id])

        # ---------------------------------------------------------------------
        # STEP 4: ENTITY SYNTHESIS 
        # ---------------------------------------------------------------------
        print("üß™ Synthesizing Entities...")
        
        valid_uids = set()
        all_uids = list(self.synthesizer._dossiers.keys())
        
        for uid in all_uids:
            # –ê. –°–ò–ù–¢–ï–ó (LLM —Å–æ–∑–¥–∞–µ—Ç —á–∏—Å—Ç–æ–≤–æ–π –ø—Ä–æ—Ñ–∏–ª—å)
            result = self.synthesizer.synthesize_profile(uid)
            if not result:
                continue
                
            profile, final_uid = result
            valid_uids.add(final_uid)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            meta = self.synthesizer._metadata[final_uid]
            meta['name'] = profile.canonical_name 
            category = meta.get('category', 'UNKNOWN')
            
            # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –Ω–∞–±–ª—é–¥–µ–Ω–∏–π (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î)
            raw_observations = self.synthesizer.get_raw_observations(final_uid)

            # -----------------------------------------------------------------
            # –ë. PROJECTION (–ú–ê–¢–ï–ú–ê–¢–ò–ö–ê –°–¢–ê–¢–û–í –ò –ê–¢–û–ú–û–í)
            # -----------------------------------------------------------------
            
            # 1. –ë–∞–∑–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä –ø–æ —á–∏—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é
            # –î–æ–±–∞–≤–ª—è–µ–º inferred_atoms –≤ —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            rich_text = f"{profile.canonical_name}. {profile.summary} Traits: {', '.join(profile.personality_traits)}."
            embedding = self._get_embedding(rich_text)
            
            # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è (—á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç)
            raw_stats = self.projector.project(embedding)
            
            # 2. –ü–æ–∏—Å–∫ –ê—Ç–æ–º–æ–≤ –∏ —Ä–∞—Å—á–µ—Ç –≤–ª–∏—è–Ω–∏—è
            component_ids = []
            atom_influence = {"material": [], "vitality": [], "social": [], "cognitive": []}
            
            if self.options.project_atoms:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å: –ö–∞—Ç–µ–≥–æ—Ä–∏—è + –¢–µ–∫—Å—Ç + –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–µ –∞—Ç–æ–º—ã –æ—Ç LLM
                atom_query_text = f"{category}: {rich_text}"
                threshold = 0.65 if category == "LORE" else 0.55
                
                # –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–µ—Å—Ç—Ä—É
                found_atoms = ATOMS.classify(atom_query_text, threshold=threshold, top_k=6)
                
                if found_atoms:
                    for atom_obj, score in found_atoms:
                        component_ids.append(atom_obj.id)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∞—Ç–æ–º–∞ (–∏–∑ –∫—ç—à–∞/–º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º)
                        if hasattr(atom_obj, 'vector') and atom_obj.vector:
                             # –ï—Å–ª–∏ —ç—Ç–æ Pydantic –º–æ–¥–µ–ª—å
                            if hasattr(atom_obj.vector, 'model_dump'):
                                atom_stats = atom_obj.vector.model_dump()
                            else:
                                atom_stats = atom_obj.vector # –ï—Å–ª–∏ —É–∂–µ dict
                        else:
                            # Fallback: –ø—Ä–æ–µ–∫—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∞—Ç–æ–º–∞
                            atom_vec_text = f"{atom_obj.name} {atom_obj.description}"
                            atom_stats = self.projector.project(self._get_embedding(atom_vec_text))

                        # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –≤–ª–∏—è–Ω–∏–µ (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –Ω–∞ score —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
                        for axis, val in atom_stats.items():
                            if axis in atom_influence:
                                atom_influence[axis].append(val * score)

            # 3. –°–ª–∏—è–Ω–∏–µ (Blending) –∏ –ú–∞—Å–∫–∏ (Bias)
            ATOM_WEIGHT = 0.3
            final_stats = {}
            
            # –ú–∞—Å–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–∏–ø–∞ —Å—É—â–Ω–æ—Å—Ç–∏
            bias = TYPE_BIAS.get(category, TYPE_BIAS["UNKNOWN"])

            for axis, base_val in raw_stats.items():
                # –∞) –°–º–µ—à–∏–≤–∞–µ–º –±–∞–∑—É —Å –∞—Ç–æ–º–∞–º–∏
                atoms_vals = atom_influence.get(axis, [])
                if atoms_vals:
                    atom_avg = sum(atoms_vals) / len(atoms_vals)
                    merged_val = (base_val * (1.0 - ATOM_WEIGHT)) + (atom_avg * ATOM_WEIGHT)
                else:
                    merged_val = base_val
                
                # –±) –ü—Ä–∏–º–µ–Ω—è–µ–º BIAS
                # (axis[:3] –±–µ—Ä–µ—Ç "mat" –∏–∑ "material")
                short_key = axis[:3]
                final_stats[axis] = merged_val * bias.get(short_key, 1.0)
            
            # -----------------------------------------------------------------
            # –í. –†–û–õ–ò (–î–ª—è –∞–≥–µ–Ω—Ç–æ–≤)
            # -----------------------------------------------------------------
            role_id = None
            if category in ["AGENT", "GROUP"] and self.options.project_roles:
                role_id = self.classifier.classify(rich_text, ROLES, top_k=3)

            # -----------------------------------------------------------------
            # –ì. –°–û–•–†–ê–ù–ï–ù–ò–ï (FINAL UPSERT)
            # -----------------------------------------------------------------
            
            # Neo4j: –æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫—É –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            self.graph_builder.neo4j.upsert_molecule(
                final_uid, 
                meta['name'], 
                category,
                role_id=role_id,
                component_ids=component_ids,
                semantic_stats=final_stats
            )
            
            # Qdrant: –ø–æ–ª–Ω—ã–π –ø–µ–π–ª–æ–∞–¥
            payload = {
                **meta,
                "description": profile.summary,
                "visuals": profile.visual_traits,
                "psychology": profile.personality_traits,
                "role_desc": profile.narrative_role_desc,
                "importance": profile.importance_score,
                "raw_observations": raw_observations,
                "stats": final_stats, # <--- –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—ã
                "component_ids": component_ids,
                "source_id": source_id,
                "is_draft": False
            }
            
            # upsert –≤—ã–∑—ã–≤–∞–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é PointStruct, –ø–æ—ç—Ç–æ–º—É embedding –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω
            self.qdrant.upsert("molecules", [PointStruct(
                id=final_uid, vector=embedding, payload=payload
            )])
            
        print("‚úÖ Entity Synthesis & Projection Complete.")

        # 3. CLEANUP (–£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞ –∏–∑ –ì—Ä–∞—Ñ–∞)
        print("üßπ Cleaning up temporary nodes...")
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (merged)
        for old_id in self.synthesizer._redirect_map.keys():
            self.graph_builder.neo4j.delete_molecule(old_id)
            self.qdrant.delete(collection_name="molecules", points_selector=[old_id])
            
        # –£–¥–∞–ª—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –º—É—Å–æ—Ä (low importance)
        for uid in all_uids:
            if uid not in valid_uids and uid not in self.synthesizer._redirect_map:
                self.graph_builder.neo4j.delete_molecule(uid)
                self.qdrant.delete(collection_name="molecules", points_selector=[uid])

        # ---------------------------------------------------------------------
        # STEP 5: EPISODE SYNTHESIS (–õ–µ—Ç–æ–ø–∏—Å—å)
        # ---------------------------------------------------------------------
        print("üìú Synthesizing Chronicles...")
        for loc_id in self.synthesizer._scene_dossiers:
            
            # 1. –°–ò–ù–¢–ï–ó (LLM –ø–∏—à–µ—Ç —Ö—Ä–æ–Ω–∏–∫—É –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ê—Ä—Ö–µ—Ç–∏–ø)
            generated_episodes = self.synthesizer.synthesize_episodes_for_loc(loc_id)
            
            for ep_data in generated_episodes:
                # –§–∏–ª—å—Ç—Ä —à—É–º–∞
                if ep_data.significance_score < 3:
                    continue

                # 2. –†–ê–°–ß–ï–¢ –ò–ì–†–û–í–û–ô –ú–ê–¢–ï–ú–ê–¢–ò–ö–ò (GameMath)
                
                # –ê. –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
                # "The Battle of the Dark Hall..."
                full_text = f"{ep_data.title}. {ep_data.summary}"
                embedding = self._get_embedding(full_text)
                raw_vector_stats = self.projector.project(embedding)
                
                # –ë. –ü–æ–∏—Å–∫ –ê—Ç–æ–º–æ–≤ (–°–æ–±—ã—Ç–∏–π–Ω—ã–π —Å–æ—Å—Ç–∞–≤)
                # –ü–æ–∂–∞—Ä = Atom(Fire). –ë–∏—Ç–≤–∞ –Ω–∞ –º–æ—Å—Ç—É = Atom(Stone), Atom(Void).
                # –≠—Ç–æ –¥–æ–±–∞–≤–∏—Ç –Ω—é–∞–Ω—Å–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä.
                atom_influence = {"material": [], "vitality": [], "social": [], "cognitive": []}
                
                if self.options.project_atoms:
                    # –ò—â–µ–º –∞—Ç–æ–º—ã –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ —Å–æ–±—ã—Ç–∏—è
                    atom_query = f"EVENT: {full_text}"
                    found_atoms = ATOMS.classify(atom_query, threshold=0.6, top_k=3)
                    
                    if found_atoms:
                        for atom_obj, score in found_atoms:
                            # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∞—Ç–æ–º–∞ (–∏–∑ –∫—ç—à–∞ –∏–ª–∏ –ø—Ä–æ–µ–∫—Ü–∏–∏)
                            if hasattr(atom_obj, 'vector') and atom_obj.vector:
                                if hasattr(atom_obj.vector, 'model_dump'):
                                    a_stats = atom_obj.vector.model_dump()
                                else:
                                    a_stats = atom_obj.vector
                            else:
                                # Fallback projection
                                a_stats = self.projector.project(
                                    self._get_embedding(f"{atom_obj.name} {atom_obj.description}")
                                )

                            # –í–∑–≤–µ—à–∏–≤–∞–µ–º –≤–ª–∏—è–Ω–∏–µ
                            for axis, val in a_stats.items():
                                if axis in atom_influence:
                                    atom_influence[axis].append(val * score)

                # –í. FINAL MATH (Blending + Archetype Bias)
                # –ü–µ—Ä–µ–¥–∞–µ–º archetype (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'conflict_physical') –≤ GameMath
                final_stats = GameMath.calculate_stats(
                    base_vector_stats=raw_vector_stats,
                    atom_influence=atom_influence,
                    category=ep_data.archetype, # <--- –ö–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç! –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
                    is_event=True 
                )
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (—É–≤–∏–¥–∏–º, –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä–∞ "—Ä–∞–∑—ä–µ–∑–∂–∞—é—Ç—Å—è" –æ—Ç 0.5)
                print(f"      üìä Event '{ep_data.title}' ({ep_data.archetype}): {final_stats}")

                # 3. LINKING TO NEO4J (Upsert)
                # –õ–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (start_tick)
                tick_window = 10 
                find_query = """
                MATCH (e:Episode)-[:HAPPENED_AT]->(l:Location {id: $lid})
                WHERE abs(e.start_tick - $my_tick) < $window
                RETURN e.id AS id
                LIMIT 1
                """
                
                existing_ep_id = None
                with self.graph_builder.neo4j.driver.session() as session:
                    res = session.run(find_query, lid=loc_id, my_tick=ep_data.start_tick, window=tick_window).single()
                    if res: existing_ep_id = res["id"]

                target_id = existing_ep_id if existing_ep_id else str(uuid.uuid4())
                
                # –û–±–Ω–æ–≤–ª—è–µ–º Neo4j
                # –í–ê–ñ–ù–û: –ú—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º final_stats –≤ Neo4j —Ç–æ–∂–µ, –µ—Å–ª–∏ —Å—Ö–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç (semantic_stats)
                if existing_ep_id:
                    self.graph_builder.neo4j.driver.execute_query(
                        """
                        MATCH (e:Episode {id: $eid}) 
                        SET e.summary = $sum, e.name = $tit, 
                            e.archetype = $arch, e.semantic_stats = $stats
                        """,
                        eid=target_id, sum=ep_data.summary, tit=ep_data.title, 
                        arch=ep_data.archetype, stats=final_stats
                    )
                else:
                    self.graph_builder.neo4j.upsert_episode(
                        uid=target_id,
                        name=ep_data.title,
                        summary=ep_data.summary,
                        start_tick=ep_data.start_tick,
                        location_id=loc_id
                    )
                    # –î–æ–ø–∏—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç—ã –∏ —Ç–∏–ø –æ—Ç–¥–µ–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º (–µ—Å–ª–∏ upsert_episode –ø—Ä–æ—Å—Ç–µ–Ω—å–∫–∏–π)
                    self.graph_builder.neo4j.driver.execute_query(
                        "MATCH (e:Episode {id: $eid}) SET e.archetype = $arch, e.semantic_stats = $stats",
                        eid=target_id, arch=ep_data.archetype.value, stats=final_stats
                    )

                # 4. INDEXING QDRANT
                payload = {
                    "name": ep_data.title,
                    "description": ep_data.summary,
                    "type": "episode",
                    "archetype": ep_data.archetype, # –í–∞–∂–Ω–æ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    "tags": ep_data.dominant_tags,
                    "participants": ep_data.key_participants,
                    "significance": ep_data.significance_score,
                    "source_loc_id": loc_id,
                    "source_id": source_id,
                    "stats": final_stats # <--- –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
                }
                
                self.qdrant.upsert("chronicle", [PointStruct(
                    id=target_id,
                    vector=embedding,
                    payload=payload
                )])

        print("‚úÖ Chronicles synthesized and projected.")

        # 4. GLOBAL NORMALIZATION (–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ)
        if self.options.project_atoms:
             self._normalize_global_stats()

        # 2. NARRATIVE ARC DETECTION
        if not self.options.detect_arcs:
            return

        print("üïµÔ∏è‚Äç‚ôÇÔ∏è Running Narrative Arc Detection (Hybrid)...")
        
        # –ê. –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5-10 —ç–ø–∏–∑–æ–¥–æ–≤)
        recent_episodes = self._fetch_recent_chronicle_events(limit=6)
        
        if not recent_episodes:
            print("   ‚ö†Ô∏è Not enough data for arc detection.")
            return

        # –ë. –§–æ—Ä–º–∏—Ä—É–µ–º "–ò—Å—Ç–æ—Ä–∏—é" –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        story_text = self._compose_safe_context(recent_episodes, max_tokens=2500)
        
        # –í. –ì–∏–±—Ä–∏–¥–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        # 1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—Ö–æ–¥–∏—Ç Top-K –ø–æ—Ö–æ–∂–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤ (ARCS).
        # 2. LLM –ø–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –ò–º—è + –û–ø–∏—Å–∞–Ω–∏–µ) –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π.
        # –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç —Ç–æ–∫–µ–Ω—ã –∏ –¥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ –ø–æ—Ä–æ–≥ 0.65.
        
        detected_arc_id = self.classifier.classify(
            query_text=story_text,
            registry=ARCS,       # –†–µ–µ—Å—Ç—Ä —Å—é–∂–µ—Ç–Ω—ã—Ö –∞—Ä–æ–∫
            threshold_high=0.65, # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω –Ω–∞ 85% ‚Äî –≤–µ—Ä–∏–º —Å—Ä–∞–∑—É
            threshold_low=0.35,  # –ï—Å–ª–∏ —Å–æ–º–Ω–µ–Ω–∏—è (0.35-0.85) ‚Äî —Å–ø—Ä–∞—à–∏–≤–∞–µ–º LLM
            top_k=3              # –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º —Ç–æ–ø-3 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        )

        if detected_arc_id:
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —à–∞–±–ª–æ–Ω–∞ –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –ª–æ–≥–∞
            arc_template = ARCS.get(detected_arc_id)
            arc_name = arc_template.name if arc_template else "Unknown Arc"
            
            print(f"   üé≠ DETECTED NARRATIVE ARC: '{arc_name}' (ID: {detected_arc_id})")
            
            # –ì. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ò–Ω—Å—Ç–∞–Ω—Å –ê—Ä–∫–∏ –≤ –ì—Ä–∞—Ñ
            instance_id = str(uuid.uuid4())
            self.graph_builder.neo4j.upsert_narrative_instance(
                instance_id, 
                detected_arc_id, 
                f"{arc_name} (Auto-detected)"
            )

            # –î. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
            arc_vec = self._get_embedding(story_text) # –í–µ–∫—Ç–æ—Ä –∏—Å—Ç–æ—Ä–∏–∏, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—à–ª–∏ –∞—Ä–∫—É
            
            self.qdrant.upsert("narrative_instances", [PointStruct(
                id=instance_id,
                vector=arc_vec,
                payload={
                    "name": f"{arc_name} Instance",
                    "template_id": detected_arc_id,
                    "description": "Auto-detected narrative arc based on recent events.",
                    "tick": recent_episodes[-1]['tick'] if recent_episodes else 0,
                    "involved_episodes": [ep['id'] for ep in recent_episodes]
                }
            )])
            print(f"      üíæ Saved Arc Instance to Qdrant.")

            # –ï. –õ–∏–Ω–∫—É–µ–º –≠–ø–∏–∑–æ–¥—ã –∫ —ç—Ç–æ–π –ê—Ä–∫–µ
            count = 0
            for ep in recent_episodes:
                self.graph_builder.neo4j.link_event_to_arc(ep['id'], instance_id)
                count += 1
            
            print(f"      üîó Linked {count} episodes to the arc.")
                
    def _normalize_global_stats(self):
        print("   ‚öñÔ∏è  Running Global Stat Normalization...")
        
        # 1. –í—ã–≥—Ä—É–∂–∞–µ–º –í–°–ï –º–æ–ª–µ–∫—É–ª—ã (—á–µ—Ä–µ–∑ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥ –∏–ª–∏ –ª–∏–º–∏—Ç)
        # –î–ª—è –æ–¥–Ω–æ–π –∫–Ω–∏–≥–∏ 10k —Ç–æ—á–µ–∫ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è RAM.
        try:
            scroll_result = self.qdrant.scroll(
                collection_name="molecules",
                limit=10_000, 
                with_payload=True,
                with_vectors=True
            )
            points = scroll_result[0]
            if not points:
                return
        except Exception as e:
            logging.error(f"      ‚ö†Ô∏è Failed to fetch points for normalization: {e}", exc_info=True)
            return

        print(f"      üìä Analyzing {len(points)} entities...")

        # 2. –°–æ–±–∏—Ä–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –æ—Å—è–º
        axes_data = {"material": [], "vitality": [], "social": [], "cognitive": []}
        
        for p in points:
            stats = p.payload.get("stats", {})
            for axis in axes_data:
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                val = stats.get(axis, 0.0)
                axes_data[axis].append(val)

        # 3. –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã (Min/Max) –¥–ª—è –∫–∞–∂–¥–æ–π –æ—Å–∏
        bounds = {}
        for axis, values in axes_data.items():
            if not values: continue
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏ (2% –∏ 98%), —á—Ç–æ–±—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∫–∏–µ –≤—ã–±—Ä–æ—Å—ã
            v_min = np.percentile(values, 2)
            v_max = np.percentile(values, 98)
            bounds[axis] = (v_min, v_max)
            print(f"      Axis '{axis}': range [{v_min:.2f}, {v_max:.2f}]")

        # 4. –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º
        updated_points = []
        
        for p in points:
            old_stats = p.payload.get("stats", {})
            new_stats = {}
            
            for axis, (v_min, v_max) in bounds.items():
                val = old_stats.get(axis, 0.0)
                
                if v_max - v_min < 0.01:
                    new_val = val # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å, –µ—Å–ª–∏ –≤—Å–µ —Ä–∞–≤–Ω—ã
                else:
                    # Min-Max Scaling
                    scaled = (val - v_min) / (v_max - v_min)
                    # Clip (0.05 - 0.95) –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã
                    new_val = 0.05 + (scaled * 0.9)
                    new_val = float(np.clip(new_val, 0.0, 1.0))
                
                new_stats[axis] = round(new_val, 3)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º payload
            p.payload["stats"] = new_stats
            
            # –î—É–±–ª–∏—Ä—É–µ–º —Å—Ç–∞—Ç—ã –≤ Neo4j (—ç—Ç–æ –º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –Ω—É–∂–Ω–æ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏)
            # TODO: –ï—Å–ª–∏ –±–∞–∑–∞ –±–æ–ª—å—à–∞—è, –ª—É—á—à–µ –¥–µ–ª–∞—Ç—å batch update –≤ Neo4j –æ—Ç–¥–µ–ª—å–Ω–æ.
            # –î–ª—è "–ê–ª–∏—Å—ã" –ø–æ–π–¥–µ—Ç –∏ —Ç–∞–∫.
            self.graph_builder.neo4j.upsert_molecule(
                p.id, 
                p.payload["name"], 
                p.payload.get("type", 'undefined'), 
                semantic_stats=new_stats
            )
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è Qdrant batch update
            updated_points.append(PointStruct(
                id=p.id, 
                vector=p.vector, # –í–µ–∫—Ç–æ—Ä –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è, –Ω–æ Qdrant —Ç—Ä–µ–±—É–µ—Ç (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ set_payload)
                payload=p.payload
            ))

        # 5. Batch Update Qdrant
        # –ú–µ—Ç–æ–¥ set_payload —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ upsert, –µ—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–µ –º–µ–Ω—è—é—Ç—Å—è, 
        # –Ω–æ upsert –ø—Ä–æ—â–µ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.
        if updated_points:
             # –ß—Ç–æ–±—ã –Ω–µ –≥–æ–Ω—è—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ —Ç—É–¥–∞-—Å—é–¥–∞, –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å client.set_payload
             # –ù–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã:
             # self.qdrant.upsert("molecules", updated_points) 
             
             # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –∏—Å–ø–æ–ª—å–∑—É–µ–º overwrite_payload
             for p in points:
                 self.qdrant.overwrite_payload(
                     collection_name="molecules",
                     payload=p.payload,
                     points=[p.id]
                 )
             print(f"      ‚úÖ Normalized {len(points)} entities globally.")

    def _compose_safe_context(self, events: List[dict], max_tokens: int = 1500) -> str:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é, –≥—Ä—É–ø–ø–∏—Ä—É—è Beats –≤ –°—Ü–µ–Ω—ã.
        –§–æ—Ä–º–∞—Ç:
        [SCENE: The Rabbit Hole]
        - Alice falls down (Event)
        - She sees maps and jars (Event)
        ...
        """
        grouped_lines = []
        current_loc = None
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –°—Ç–∞—Ä–æ–≥–æ –∫ –ù–æ–≤–æ–º—É
        for evt in events:
            loc_name = evt.get('loc_name', 'Unknown Place')
            
            # –ï—Å–ª–∏ –ª–æ–∫–∞—Ü–∏—è —Å–º–µ–Ω–∏–ª–∞—Å—å, –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ü–µ–Ω—ã
            if loc_name != current_loc:
                grouped_lines.append(f"\n[SCENE: {loc_name}]")
                current_loc = loc_name
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å–æ–±—ã—Ç–∏—è
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∞—Ä—Ö–µ—Ç–∏–ø, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫–∞–∫ —Ç–µ–≥ [ATTACK], [MOVE]
            tag = f"[{evt['archetype']}] " if evt.get('archetype') else ""
            line = f"- {tag}{evt['name']}: {evt['description']}"
            
            grouped_lines.append(line)

        # –¢–µ–ø–µ—Ä—å –æ–±—Ä–µ–∑–∞–µ–º —Å –∫–æ–Ω—Ü–∞ (—á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Å–≤–µ–∂–∏–µ —Å–æ–±—ã—Ç–∏—è), –µ—Å–ª–∏ –≤—ã–ª–µ–∑–ª–∏ –∑–∞ –ª–∏–º–∏—Ç
        # –ù–æ —Ç–∞–∫ –∫–∞–∫ –º—ã —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∏ —Å–ø–∏—Å–æ–∫ linear, –Ω–∞–º –Ω—É–∂–Ω–æ —Ö–∏—Ç—Ä–æ –æ–±—Ä–µ–∑–∞—Ç—å –Ω–∞—á–∞–ª–æ.
        
        final_text = ""
        # –°–æ–±–∏—Ä–∞–µ–º —Å –∫–æ–Ω—Ü–∞, –ø–æ–∫–∞ –≤–ª–∞–∑–∏—Ç
        buffer = []
        current_tokens = 0
        
        for line in reversed(grouped_lines):
            tokens = len(self.tokenizer(line)) + 1
            if current_tokens + tokens > max_tokens:
                break
            buffer.append(line)
            current_tokens += tokens
            
        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        return "\n".join(reversed(buffer))

    # TODO: –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –ë–î
    def _fetch_recent_chronicle_events(self, limit: int = 5):
        """
        –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≠–ø–∏–∑–æ–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—é–∂–µ—Ç–∞.
        –í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ —Å–æ–∑–¥–∞—Ç—å —Å–≤—è–∑–∏ –≤ –≥—Ä–∞—Ñ–µ.
        """
        query = """
        MATCH (e:Episode)
        RETURN e.id AS id, e.name AS name, e.summary AS description, e.start_tick AS tick
        ORDER BY e.start_tick DESC
        LIMIT $limit
        """
        events = []
        with self.graph_builder.neo4j.driver.session() as session:
            result = session.run(query, limit=limit)
            for record in result:
                data = record.data()
                # –•–∞–∫ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–Ω –∏—â–µ—Ç 'loc_name', –Ω–æ –¥–ª—è –≠–ø–∏–∑–æ–¥–∞ –∏–º—è –∏ –µ—Å—Ç—å –ª–æ–∫–∞—Ü–∏—è)
                data['loc_name'] = data['name'] 
                events.append(data)
        
        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º [Old -> New] –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è LLM
        return events[::-1]

    def _process_relationships(self, relationships: List[ExtractedRelationship], 
                               full_registry: Dict[str, str], 
                               current_loc_id: str):
        
        print(f"   üîó Processing {len(relationships)} raw links...")
        
        last_subject_id = None
        
        for rel in relationships:
            try:
                # 1. Resolve IDs (–∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ)
                subj_id = self._resolve_entity_id(rel.subject_name, full_registry, current_loc_id, last_subject_id)
                if subj_id: last_subject_id = subj_id
                
                obj_id = self._resolve_entity_id(rel.target_name, full_registry, current_loc_id, last_subject_id)
                
                if not subj_id or not obj_id:
                    continue

                # === –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: SANITY CHECK ===
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞
                # (GraphBuilder –Ω–∞–ø–æ–ª–Ω–∏–ª self.synthesizer._metadata –Ω–∞ Pass 2)
                subj_meta = self.synthesizer._metadata.get(subj_id, {})
                obj_meta = self.synthesizer._metadata.get(obj_id, {})
                
                subj_type = subj_meta.get('category', 'UNKNOWN')
                obj_type = obj_meta.get('category', 'UNKNOWN')
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—ã—Ä–æ–π —Ç–∏–ø —Å–≤—è–∑–∏ (—á–µ—Ä–µ–∑ –º–∞–ø–ø–µ—Ä –∏–ª–∏ LLM output)
                # –î–æ–ø—É—Å—Ç–∏–º, LLM –≤–µ—Ä–Ω—É–ª–∞ category="PHYSICAL" description="inside"
                from src.ingestion.mappers import RELATIONS
                raw_rel_type = RELATIONS.map_container(rel.description) or "RELATED_TO"
                
                # –ï—Å–ª–∏ LLM —É–∂–µ –ø–æ–º–µ—Ç–∏–ª–∞ —ç—Ç–æ –∫–∞–∫ MENTAL –≤ –ø—Ä–æ–º–ø—Ç–µ
                if rel.category == "MENTAL":
                    raw_rel_type = "THINKS_OF"

                # –ó–ê–ü–£–°–ö–ê–ï–ú –°–ê–ù–ò–¢–ê–ô–ó–ï–†
                final_rel_type = RelationshipSanitizer.validate_and_fix(
                    subj_type, obj_type, raw_rel_type, rel.description
                )
                
                print(f"      Link: {subj_meta.get('name')} ({subj_type}) -[{final_rel_type}]-> {obj_meta.get('name')} ({obj_type})")

                # === –°–û–•–†–ê–ù–ï–ù–ò–ï –í NEO4J ===
                
                if final_rel_type == "LOCATED_AT":
                    # –§–∏–∑–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ
                    self.graph_builder.neo4j.link_possession(item_id=subj_id, owner_id=obj_id, rel_type="LOCATED_AT")
                    
                elif final_rel_type == "POSSESSES":
                    self.graph_builder.neo4j.link_possession(item_id=obj_id, owner_id=subj_id, rel_type="EQUIPPED")
                    
                elif final_rel_type == "NEAR":
                    self.graph_builder.neo4j.link_social(subj_id, obj_id, "NEAR", 1.0)
                    
                elif final_rel_type in ["THINKS_OF", "RECALLS", "MENTIONED_BY"]:
                    # –≠—Ç–æ –º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Å–≤—è–∑—å, –æ–Ω–∞ –ù–ï –¥–≤–∏–≥–∞–µ—Ç —Ñ–∏–≥—É—Ä–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–µ
                    self.graph_builder.neo4j.driver.execute_query(
                        "MATCH (a {id: $aid}), (b {id: $bid}) MERGE (a)-[:THINKS_OF]->(b)",
                        aid=subj_id, bid=obj_id
                    )

                # B. SOCIAL (–≠–º–æ—Ü–∏–∏, –ò–µ—Ä–∞—Ä—Ö–∏—è)
                elif final_rel_type == "SOCIAL":
                    # "hates", "loves", "serves"
                    rel_type = RELATIONS.map_social(rel.description) or "NEUTRAL"
                    # –¢—É—Ç –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è intensity (–ø–æ–∫–∞ 1.0)
                    self.graph_builder.neo4j.link_social(subj_id, obj_id, rel_type, intensity=1.0)

                # C. SPATIAL (–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ)
                elif final_rel_type == "SPATIAL":
                    # –î–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞:
                    # 1. "–°—É–Ω–¥—É–∫ —Å—Ç–æ–∏—Ç –ù–ê —Å—Ç–æ–ª–µ" (Containment)
                    # 2. "–î–≤–µ—Ä—å –≤–µ–¥–µ—Ç –í –∫–æ—Ä–∏–¥–æ—Ä" (Topology)
                    
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–Ω—è—Ç—å, —ç—Ç–æ Containment? ("on", "in", "under")
                    cont_type = RELATIONS.map_container(rel.description)
                    
                    if cont_type in ["LOCATED_AT", "IS_INSIDE"]:
                         # (Object)-[:LOCATED_AT]->(Subject) "Chest on Table" -> Table is parent
                         # –¢—É—Ç –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º: "X is on Y". X is item, Y is container.
                         # Subject (X) -> Target (Y)
                         self.graph_builder.neo4j.link_possession(item_id=subj_id, owner_id=obj_id, rel_type=cont_type)
                    else:
                        # –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å (–ø—Ä–æ—Ö–æ–¥)?
                        edge_type = RELATIONS.map_edge(rel.description)
                        if edge_type:
                             self.graph_builder.neo4j.link_locations(subj_id, obj_id, edge_type)

                # D. KNOWLEDGE (–°–µ–∫—Ä–µ—Ç—ã)
                elif final_rel_type == "KNOWLEDGE":
                    # "Knows about the murder"
                    # –ó–¥–µ—Å—å obj_id –º–æ–∂–µ—Ç –±—ã—Ç—å Event –∏–ª–∏ Secret. 
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–∞–ø–∏—Ç—Å—è –ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ KNOWS_SECRET
                    rel_type = RELATIONS.map_container(rel.description)
                    if rel_type == "KNOWS_SECRET":
                         self.graph_builder.neo4j.link_knowledge(subj_id, obj_id)

            except Exception as e:
                logging.error(f"      ‚ùå Link Error '{rel.subject_name}' -> '{rel.target_name}': {e}", exc_info=True)

    def _resolve_entity_id(self, name_query: str, 
                           registry: Dict[str, str], 
                           current_loc_id: str,
                           context_agent_id: Optional[str] = None) -> Optional[str]:
        
        clean = name_query.lower().strip()
        
        # 1. PRONOUNS (–ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è)
        if clean in ["he", "she", "they", "it", "him", "her"]:
            if context_agent_id:
                # print(f"      üîÑ Resolved Pronoun '{clean}' -> {context_agent_id}")
                return context_agent_id
            return None # –ù–µ –∑–Ω–∞–µ–º, –æ –∫–æ–º —Ä–µ—á—å

        # 2. CONTEXT (–ú–µ—Å—Ç–æ)
        if clean in ["here", "this place", "room", "area", "ground"]:
            return current_loc_id
            
        # 3. DIRECT LOOKUP (–†–µ–µ—Å—Ç—Ä)
        # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ("Alice")
        if clean in registry:
            return registry[clean]
            
        # 4. PARTIAL LOOKUP (–ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ "The Key", –∞ –≤ —Ä–µ–µ—Å—Ç—Ä–µ "Golden Key")
        # –≠—Ç–æ–≥–æ –Ω–µ –±—ã–ª–æ, –Ω–æ —ç—Ç–æ –≤–∞–∂–Ω–æ!
        for reg_name, uuid_val in registry.items():
            if clean in reg_name or reg_name in clean:
                # –û–ø–∞—Å–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤, –Ω–æ –¥–ª—è "key" -> "golden key" —Å—Ä–∞–±–æ—Ç–∞–µ—Ç
                if len(clean) > 3: 
                    return uuid_val

        # 5. FUZZY DB SEARCH (–ü–æ—Å–ª–µ–¥–Ω–∏–π —Ä—É–±–µ–∂)
        # –ï—Å–ª–∏ —ç—Ç–æ —Å–æ–≤—Å–µ–º –Ω–æ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä—É—é Macro-Pass –ø—Ä–æ–ø—É—Å—Ç–∏–ª
        fuzzy = self.graph_builder.neo4j.fuzzy_search_molecule(name_query) 
        if fuzzy:
            return fuzzy
            
        return None

