import logging
import re
import numpy as np
from typing import List, Tuple, Any, Optional
from pydantic import BaseModel, Field
from rapidfuzz import fuzz

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º PrivateAttr –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ (LLM, Embedder) –≤–Ω—É—Ç—Ä–∏ Pydantic –º–æ–¥–µ–ª–µ–π
from llama_index.core.bridge.pydantic import PrivateAttr
from llama_index.core.node_parser import NodeParser, SentenceSplitter
from llama_index.core.schema import BaseNode, TextNode, Document
from llama_index.core import PromptTemplate
from src.custom_program import LocalStructuredProgram
from src.config import config

# === –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• ===

class SceneBoundary(BaseModel):
    """
    –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –≥—Ä–∞–Ω–∏—Ü—ã —Å—Ü–µ–Ω—ã.
    –°–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ CoT (summary), Anchors (snippets) –∏ Metadata (label).
    """
    # 1. CHAIN OF THOUGHT (–°–Ω–∞—á–∞–ª–∞ LLM –¥—É–º–∞–µ—Ç –æ —Å–æ–±—ã—Ç–∏–∏)
    event_summary: str = Field(description="Brief summary of the event starting here.")
    
    # 2. METADATA (–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏)
    context_label: str = Field(description="Short title for this section (e.g. 'Alice falls down').")
    
    # 3. ANCHORS (–Ø–∫–æ—Ä—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ)
    pre_context: str = Field(description="The LAST sentence (verbatim) of the PREVIOUS scene.")
    start_snippet: str = Field(description="The FIRST sentence (verbatim) of the NEW scene.")
    
    # 4. CLASSIFICATION
    scene_type: str = Field(description="PHYSICAL, MEMORY, DREAM, DOCUMENT.")
    reason: str = Field(description="Why split here?")

class SegmentationBatch(BaseModel):
    """
    –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.
    """
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π CoT: –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤—Å–µ–≥–æ –∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ü–ï–†–ï–î —Ç–µ–º, –∫–∞–∫ –≤—ã–¥–µ–ª—è—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã
    reasoning_chain: str = Field(description="Step-by-step analysis of the narrative flow and structure.")
    boundaries: List[SceneBoundary]

class LocatorResult(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã LLM-—Å–Ω–∞–π–ø–µ—Ä–∞."""
    exact_quote: str = Field(description="The exact verbatim string found in the text.")
    is_found: bool = Field(description="True if the quote was successfully located.")
    confidence: float = Field(description="Certainty level (0.0-1.0).")
    reasoning: str = Field(description="Brief explanation of how the match was found (e.g. 'Corrected spelling error').")


# === –°–ü–õ–ò–¢–¢–ï–†–´ ===

class AdaptiveMicroSplitter(NodeParser):
    """
    –£–º–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞.
    """
    # 1. –û–±—ä—è–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è (Pydantic Fields)
    min_tokens: int = Field(default=500, description="Minimum chunk size")
    max_tokens: int = Field(default=2000, description="Maximum chunk size")
    base_threshold: float = Field(default=0.4, description="Semantic split threshold")
    
    # 2. –û–±—ä—è–≤–ª—è–µ–º –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
    # –û–Ω–∏ –Ω–µ –±—É–¥—É—Ç –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å—Å—è Pydantic, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ self._embedder
    _embedder: Any = PrivateAttr()
    _tokenizer: Any = PrivateAttr()
    _sentence_splitter: SentenceSplitter = PrivateAttr()

    def __init__(
        self, 
        embedder, 
        tokenizer, 
        min_tokens: int = 500, 
        max_tokens: int = 2000,
        base_threshold: float = 0.4,
        **kwargs
    ):
        # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª—è –≤ super().__init__ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        super().__init__(
            min_tokens=min_tokens,
            max_tokens=max_tokens,
            base_threshold=base_threshold,
            **kwargs
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç—ã –≤ –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã (—á–µ—Ä–µ–∑ underscore)
        self._embedder = embedder
        self._tokenizer = tokenizer
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å–ø–ª–∏—Ç—Ç–µ—Ä
        self._sentence_splitter = SentenceSplitter(chunk_size=max_tokens)

    def _map_sentence_offsets(self, original_text: str, sentences: List[str]) -> List[Tuple[int, int]]:
        offsets = []
        cursor = 0
        for sent in sentences:
            start = original_text.find(sent, cursor)
            if start == -1:
                start = cursor 
            end = start + len(sent)
            offsets.append((start, end))
            cursor = end 
        return offsets

    def _calc_distances(self, embeddings: List[List[float]]) -> List[float]:
        distances = []
        for i in range(len(embeddings) - 1):
            vec_a = np.array(embeddings[i])
            vec_b = np.array(embeddings[i+1])
            similarity = np.dot(vec_a, vec_b)
            distances.append(1.0 - similarity)
        return distances

    def get_nodes_from_documents(self, documents: List[Document], **kwargs) -> List[BaseNode]:
        final_nodes = []
        
        for doc in documents:
            full_text = doc.text
            # –û–±—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º—É –∞—Ç—Ä–∏–±—É—Ç—É
            sentences = self._sentence_splitter.split_text(full_text)
            if not sentences: continue
            
            sent_spans = self._map_sentence_offsets(full_text, sentences)
            
            print(f"   üî¨ Adaptive Split: Analyzing {len(sentences)} sentences...")

            try:
                # –û–±—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º—É –∞—Ç—Ä–∏–±—É—Ç—É
                embeddings = self._embedder.get_text_embedding_batch(sentences)
            except:
                embeddings = [self._embedder.get_text_embedding(s) for s in sentences]

            if len(embeddings) < 2:
                node = TextNode(text=full_text)
                node.metadata["start_char_idx"] = 0
                node.metadata["end_char_idx"] = len(full_text)
                final_nodes.append(node)
                continue

            distances = self._calc_distances(embeddings)
            
            chunk_start_idx = 0  
            current_tokens = 0
            
            i = 0
            while i < len(sentences):
                # –û–±—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º—É –∞—Ç—Ä–∏–±—É—Ç—É
                token_count = len(self._tokenizer(sentences[i])) if self._tokenizer else len(sentences[i]) // 4
                current_tokens += token_count
                
                is_last_sentence = (i == len(sentences) - 1)
                should_split = False
                dist = distances[i] if i < len(distances) else 0.0
                
                if current_tokens >= self.max_tokens:
                    should_split = True
                
                elif current_tokens >= self.min_tokens:
                    progress = (current_tokens - self.min_tokens) / (self.max_tokens - self.min_tokens)
                    dynamic_threshold = self.base_threshold * (1.2 - (0.7 * progress))
                    if dist > dynamic_threshold:
                        should_split = True

                if should_split or is_last_sentence:
                    real_start = sent_spans[chunk_start_idx][0]
                    real_end = sent_spans[i][1]
                    
                    chunk_text = full_text[real_start:real_end]
                    
                    node = TextNode(text=chunk_text)
                    node.metadata["start_char_idx"] = real_start
                    node.metadata["end_char_idx"] = real_end
                    
                    final_nodes.append(node)
                    
                    chunk_start_idx = i + 1
                    current_tokens = 0
                
                i += 1
                
        return final_nodes

    def _parse_nodes(self, nodes: List[BaseNode], **kwargs) -> List[BaseNode]:
        return self.get_nodes_from_documents([Document(text=n.get_content()) for n in nodes])


class SemanticSceneSplitter(NodeParser):
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    window_size: int = Field(default=25_000, description="Context window size")
    min_scene_len: int = Field(default=1_000, description="Min chars per scene")
    
    # –ü—Ä–∏–≤–∞—Ç–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
    _llm: Any = PrivateAttr()
    _segment_program: LocalStructuredProgram = PrivateAttr()
    _locator_program: LocalStructuredProgram = PrivateAttr()

    def __init__(self, llm, window_size: int = 25_000, **kwargs):
        super().__init__(window_size=window_size, **kwargs)
        self._llm = llm
        
        # 1. –ü–†–û–ì–†–ê–ú–ú–ê –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò (–°–æ–±—ã—Ç–∏–π–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
        seg_prompt = PromptTemplate(
            "Analyze the text stream for Narrative Events.\n"
            "Identify where the scene significantly changes (Location, Time, or Mood shift).\n\n"
            "STRATEGY:\n"
            "1. Analyze the narrative flow (reasoning_chain).\n"
            "2. Identify the EXACT sentences that separate events.\n"
            "3. Generate a descriptive Title (context_label) for each new section.\n\n"
            "OUTPUT FORMAT:\n"
            "- 'reasoning_chain': Step-by-step thinking process.\n"
            "- 'pre_context': The last sentence of the old scene.\n"
            "- 'start_snippet': The first sentence of the new scene.\n"
            "- 'context_label': A short name for the scene.\n"
            "- 'event_summary': What happens in this scene.\n\n"
            "TEXT STREAM:\n{text}\n"
        )
        self._segment_program = LocalStructuredProgram(
            output_cls=SegmentationBatch,
            llm=self._llm,
            prompt=seg_prompt,
            verbose=True,
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )

        # 2. –ü–†–û–ì–†–ê–ú–ú–ê-–õ–û–ö–ê–¢–û–† (Fallback)
        # –ï—Å–ª–∏ Fuzzy –Ω–µ –Ω–∞—à–µ–ª —Ü–∏—Ç–∞—Ç—É, —ç—Ç–æ—Ç –∞–≥–µ–Ω—Ç –Ω–∞–π–¥–µ—Ç –µ—ë —Ç–æ—á–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ
        loc_prompt = PromptTemplate(
            "I have a quote that might be slightly hallucinated or misformatted.\n"
            "Find the equivalent text in the provided Source Passage.\n\n"
            "HALLUCINATED QUOTE: \"{quote}\"\n\n"
            "SOURCE PASSAGE:\n{text}\n\n"
            "TASK:\n"
            "1. Find the best matching substring in the Source Passage.\n"
            "2. Return the EXACT text from the source.\n"
            "3. If absolutely not found, set is_found=False.\n"
        )
        self._locator_program = LocalStructuredProgram(
            output_cls=LocatorResult, 
            llm=self._llm,
            prompt=loc_prompt,
            verbose=True,
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )

    def _normalize(self, text: str) -> str:
        """–£–±–∏—Ä–∞–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
        return re.sub(r'\W+', ' ', text).lower().strip()

    def _skeleton_find(self, source_text: str, snippet: str, start_offset: int) -> int:
        """
        –ò—â–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–µ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –≤—Å—ë, –∫—Ä–æ–º–µ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –†–ï–ê–õ–¨–ù–´–ô –∏–Ω–¥–µ–∫—Å –Ω–∞—á–∞–ª–∞ –≤ source_text.
        """
        # 1. –°–æ–∑–¥–∞–µ–º "—Å–∫–µ–ª–µ—Ç—ã" (—Ç–æ–ª—å–∫–æ alnum, lowercase)
        # –ù–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞–ø–ø–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–∫–µ–ª–µ—Ç–∞ –Ω–∞ –∏–Ω–¥–µ–∫—Å—ã –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å –æ–∫–Ω–æ–º –ø–æ–∏—Å–∫–∞, –∞ –Ω–µ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–æ–º
        search_window = source_text[start_offset : start_offset + 5000]
        if not search_window: return -1
        
        snippet_skeleton = "".join(c for c in snippet if c.isalnum()).lower()
        if not snippet_skeleton: return -1

        # 2. –°—Ç—Ä–æ–∏–º —Å–∫–µ–ª–µ—Ç –æ–∫–Ω–∞ –∏ –∫–∞—Ä—Ç—É –∏–Ω–¥–µ–∫—Å–æ–≤
        window_skeleton = []
        map_back = [] # index in skeleton -> index in window
        
        for i, char in enumerate(search_window):
            if char.isalnum():
                window_skeleton.append(char.lower())
                map_back.append(i)
                
        window_skeleton_str = "".join(window_skeleton)
        
        # 3. –ò—â–µ–º —Å–∫–µ–ª–µ—Ç —Å–Ω–∏–ø–ø–µ—Ç–∞ –≤–Ω—É—Ç—Ä–∏ —Å–∫–µ–ª–µ—Ç–∞ –æ–∫–Ω–∞
        skeleton_idx = window_skeleton_str.find(snippet_skeleton)
        
        if skeleton_idx != -1:
            # 4. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            real_local_idx = map_back[skeleton_idx]
            return start_offset + real_local_idx
            
        return -1


    def _robust_find_index(self, window_text: str, snippet: str, search_start: int) -> int:
            """
            –ü–æ–ø—ã—Ç–∫–∞ 1: Exact
            –ü–æ–ø—ã—Ç–∫–∞ 2: Skeleton (No punctuation)
            –ü–æ–ø—ã—Ç–∫–∞ 3: Fuzzy
            –ü–æ–ø—ã—Ç–∫–∞ 4: LLM Locator
            """
            # 0. Sanity Check
            if not snippet or len(snippet) < 3: return -1
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–∫–Ω–æ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
            # –ù–æ –¥–ª—è skeleton_find –ø–µ—Ä–µ–¥–∞–µ–º offset, –æ–Ω —Å–∞–º –æ–±—Ä–µ–∂–µ—Ç
            
            # 1. EXACT MATCH
            exact = window_text.find(snippet, search_start, search_start + 5000)
            if exact != -1:
                return exact

            # 2. SKELETON MATCH (New!)
            # –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É: "Alice said," vs "Alice said" vs "Alice   said"
            skel_idx = self._skeleton_find(window_text, snippet, search_start)
            if skel_idx != -1:
                print(f"      üíÄ Skeleton match found for: '{snippet[:15]}...'")
                return skel_idx

            # 3. FUZZY SEARCH (Alignment)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º—Å—è –∫—É—Å–∫–æ–º —Ç–µ–∫—Å—Ç–∞
            search_chunk = window_text[search_start : search_start + 5000]
            
            alignment = fuzz.partial_ratio_alignment(
                snippet.lower(), 
                search_chunk.lower(),
                score_cutoff=85 
            )
            
            if alignment and alignment.score >= 85:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
                match_len = alignment.src_end - alignment.src_start
                if match_len > len(snippet) * 0.6:
                    return search_start + alignment.dest_start

            # 4. LLM FALLBACK (Locator)
            print(f"      ‚ö†Ô∏è All algos failed for: '{snippet[:30]}...'. Calling Locator.")
            
            try:
                res: LocatorResult = self._locator_program(
                    quote=snippet,
                    text=search_chunk[:2000] # –î–∞–µ–º LLM —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –∑–æ–Ω—ã –ø–æ–∏—Å–∫–∞
                )
                
                if res.is_found and res.exact_quote:
                    clean_quote = res.exact_quote.strip()
                    
                    # –ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ LLM –≤–µ—Ä–Ω—É–ª–∞ "–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é" —Ü–∏—Ç–∞—Ç—É,
                    # –°–ù–û–í–ê –ø—Ä–æ–≥–æ–Ω—è–µ–º –µ—ë —á–µ—Ä–µ–∑ Skeleton Search (–≤–¥—Ä—É–≥ LLM –æ–ø—è—Ç—å –æ—à–∏–±–ª–∞—Å—å –≤ –ø—Ä–æ–±–µ–ª–µ)
                    
                    # –ü–æ–ø—ã—Ç–∫–∞ –ê: –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ LLM
                    retry_exact = window_text.find(clean_quote, search_start, search_start + 5000)
                    if retry_exact != -1:
                        print(f"      ‚úÖ Locator Fixed: Exact match found.")
                        return retry_exact
                    
                    # –ü–æ–ø—ã—Ç–∫–∞ –ë: –°–∫–µ–ª–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ LLM
                    retry_skel = self._skeleton_find(window_text, clean_quote, search_start)
                    if retry_skel != -1:
                        print(f"      ‚úÖ Locator Fixed: Skeleton match found.")
                        return retry_skel
                        
                print(f"      ‚ùå Locator returned text '{res.exact_quote[:20]}...' but still not found.")

            except Exception as e:
                print(f"      ‚ùå Locator crashed: {e}")
                
            return -1

    def get_nodes_from_documents(self, documents: List[Document], **kwargs) -> List[BaseNode]:
        final_nodes = []
        
        for doc in documents:
            full_text = doc.text
            total_len = len(full_text)
            global_cursor = 0
            
            print(f"‚úÇÔ∏è  Smart Splitter: Processing {total_len} chars...")

            while global_cursor < total_len:
                window_end = min(global_cursor + self.window_size, total_len)
                window_text = full_text[global_cursor : window_end]
                
                if len(window_text) < self.min_scene_len:
                    # –•–≤–æ—Å—Ç
                    final_nodes.append(self._create_node(window_text, "PHYSICAL", "End", global_cursor))
                    break

                try:
                    # 1. Extract Boundaries
                    response: SegmentationBatch = self._segment_program(text=window_text)
                    
                    if not response.boundaries:
                        # –ù–µ—Ç —Å—Ü–µ–Ω? –í–µ—Å—å –∫—É—Å–æ–∫ - –æ–¥–Ω–∞ —Å—Ü–µ–Ω–∞.
                        print("   ‚è© No split detected. Advancing full window.")
                        final_nodes.append(self._create_node(window_text, "PHYSICAL", "Continuous", global_cursor))
                        global_cursor += len(window_text) # –ò–ª–∏ window_size - overlap
                        continue

                    local_cursor = 0
                    last_found_global = global_cursor
                    
                    current_meta = final_nodes[-1].metadata if final_nodes else {
                        "scene_type": "PHYSICAL", "context_label": "Intro",
                        "event_summary": 'Start of narrative'
                    }

                    for b in response.boundaries:
                        # 2. Robust Find
                        # –ò—â–µ–º start_snippet
                        found_idx = self._robust_find_index(window_text, b.start_snippet, local_cursor)
                        
                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ start_snippet, –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ pre_context (–∫–æ–Ω–µ—Ü –ø—Ä–µ–¥—ã–¥—É—â–µ–π)
                        if found_idx == -1:
                             # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                             pre_idx = self._robust_find_index(window_text, b.pre_context, local_cursor)
                             if pre_idx != -1:
                                 # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥—ã–¥—É—â–µ–π, —Ç–æ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π = –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥—ã–¥—É—â–µ–π + –¥–ª–∏–Ω–∞
                                 found_idx = pre_idx + len(b.pre_context)
                                 print(f"      ‚öì Anchored via Pre-Context: '{b.pre_context[:20]}...'")

                        if found_idx == -1:
                            print(f"      üö´ Skipped boundary (Not found): {b.start_snippet[:30]}...")
                            continue
                        
                        # –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –±–ª–∏–∑–∫–æ?
                        if found_idx < 50:
                            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—É, –Ω–æ –Ω–µ —Ä–µ–∂–µ–º (—Å–ª–∏—à–∫–æ–º –Ω–∞—á–∞–ª–æ –æ–∫–Ω–∞)
                            current_meta.update({"scene_type": b.scene_type, "context_label": b.context_label})
                            continue

                        # 3. Create Node (–° –ª–æ–≥–∏–∫–æ–π —Å–ª–∏—è–Ω–∏—è)
                        scene_text = window_text[local_cursor:found_idx]
                        
                        abs_start = global_cursor + local_cursor
                        abs_end = global_cursor + found_idx
                        
                        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê ---
                        is_too_short = len(scene_text) < self.min_scene_len
                        
                        if is_too_short and final_nodes:
                            # MERGE WITH PREVIOUS: –°—Ü–µ–Ω–∞ —Å–ª–∏—à–∫–æ–º –º–µ–ª–∫–∞—è, —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ "—Ö–≤–æ—Å—Ç" –ø—Ä–µ–¥—ã–¥—É—â–µ–π.
                            print(f"      üîó Merging short chunk ({len(scene_text)} chars) into previous scene.")
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –Ω–æ–¥—É
                            prev_node = final_nodes[-1]
                            
                            # –î–æ–∫–ª–µ–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç (LlamaIndex TextNode –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–µ–Ω—è—Ç—å .text)
                            new_text = prev_node.get_content() + "\n" + scene_text # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                            prev_node.set_content(new_text)
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–∞
                            prev_node.metadata["end_char_idx"] = abs_end
                            
                            # (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ú–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å summary –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –¥–æ–±–∞–≤–∏–≤ –∏–Ω—Ñ–æ –æ —Ö–≤–æ—Å—Ç–µ
                            prev_node.metadata["event_summary"] = prev_node.metadata.get('event_summary', '') + f" Also: {current_meta.get('event_summary', '')}"

                        elif len(scene_text) > 0:
                            # CREATE NEW: –°—Ü–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è –∏–ª–∏ —ç—Ç–æ —Å–∞–º–∞—è –ø–µ—Ä–≤–∞—è —Å—Ü–µ–Ω–∞
                            node = TextNode(text=scene_text)
                            node.metadata["scene_type"] = current_meta.get("scene_type", "PHYSICAL")
                            node.metadata["context_label"] = current_meta.get("context_label", "Narrative")
                            node.metadata["event_summary"] = current_meta.get("event_summary", "")
                            node.metadata["start_char_idx"] = abs_start
                            node.metadata["end_char_idx"] = abs_end
                            
                            final_nodes.append(node)

                        # Update State
                        local_cursor = found_idx
                        last_found_global = global_cursor + found_idx
                        
                        # Prepare for next
                        # –¢–µ–ø–µ—Ä—å –º—ã –±–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–æ–ª—å–∫–æ —á—Ç–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –≥—Ä–∞–Ω–∏—Ü—ã `b`
                        # –ò —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–π–¥—É—Ç –≤ –ú–ï–¢–ê–î–ê–ù–ù–´–ï —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞
                        try:
                            current_meta = {
                                "scene_type": b.scene_type, 
                                # –¢–µ–ø–µ—Ä—å b.context_label —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É
                                "context_label": b.context_label,
                                "event_summary": b.event_summary if hasattr(b, 'event_summary') else ''
                            }
                        except:
                            print('–Ω–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª')
                            pass

                    # 4. Advance Global Cursor
                    # –°–¥–≤–∏–≥–∞–µ–º –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–∞–π–¥–µ–Ω–Ω—É—é –≥—Ä–∞–Ω–∏—Ü—É
                    if last_found_global > global_cursor:
                        print(f"   üîÑ Advancing cursor to {last_found_global}")
                        global_cursor = last_found_global
                    else:
                        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–¥–≤–∏–≥
                        print("   ‚ö†Ô∏è No valid boundaries anchored. Advancing safe step (70%).")
                        safe_step = int(self.window_size * 0.7)
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—É—Å–æ–∫, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å
                        text_chunk = window_text[:safe_step]
                        final_nodes.append(self._create_node(text_chunk, "PHYSICAL", "Flow", global_cursor))
                        global_cursor += safe_step

                except Exception as e:
                    logging.error(f"Critical Split Error: {e}", exc_info=True)
                    global_cursor += int(self.window_size * 0.5)

        return final_nodes

    def _create_node(self, text, type_, label, start_idx):
        n = TextNode(text=text)
        n.metadata["scene_type"] = type_
        n.metadata["context_label"] = label
        n.metadata["start_char_idx"] = start_idx
        n.metadata["end_char_idx"] = start_idx + len(text)
        return n

    def _parse_nodes(self, nodes: List[BaseNode], **kwargs) -> List[BaseNode]:
        return self.get_nodes_from_documents([Document(text=n.get_content()) for n in nodes])
