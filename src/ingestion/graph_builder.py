import uuid
import logging
from typing import Dict, List, Optional, Tuple
from llama_index.core.node_parser import TokenTextSplitter
from llama_index.core import PromptTemplate
#from llama_index.core.program import LLMTextCompletionProgram
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.openai_like import OpenAILikeEmbedding
from llama_index.core.base.embeddings.base import Embedding
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from llama_index.core import Document
from llama_index.core.schema import BaseNode
from llama_index.core.schema import MetadataMode

from src.infrastructure.llama_adapter import SmartLlamaLLM
from src.infrastructure.smart_client import SmartOpenAI
from src.ingestion.scene_splitter import SemanticSceneSplitter
from src.ingestion.synthesizer import EntitySynthesizer
from src.config import config, PipelineOptions
from src.registries.all_registries import TOPOLOGIES, EVENTS
from src.database.graph_db import Neo4jConnector
from src.custom_program import LocalStructuredProgram as LLMTextCompletionProgram
from src.ingestion.classifier import HybridClassifier
from src.ingestion.graph_schemas import AssetSubtype, CausalLink, DetectedEntity, EntityBatch, GraphEvent, GraphLocation, LocationConnection, SceneBatch, SceneEventBatch, SkeletonBatch
#from src.ingestion.mappers import RELATIONS
from src.ingestion.semantic_projector import SemanticProjector


class GraphBuilder:
    def __init__(self, synthesizer=None, options: PipelineOptions = PipelineOptions()):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GraphBuilder —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–ø—Ü–∏–π –ø–∞–π–ø–ª–∞–π–Ω–∞.
        """
        # 1. Models
        # self.llm = OpenAILike(model=config.llm.model_name, 
        #                   api_base=config.llm.base_url,
        #                   temperature=0.1)
        # 1. –Ø–î–†–û: Smart Client (–¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∏ Structured Outputs)
        self.smart_client = SmartOpenAI(
            api_key=config.llm.api_key,
            base_url=config.llm.base_url,
            cache_dir="cache/global_llm"
        )
        
        # 2. –û–ë–ï–†–¢–ö–ê: LlamaIndex LLM (–¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤)
        self.llm = SmartLlamaLLM(
            model_name=config.llm.model_name,
            smart_client=self.smart_client
        )
        self.embedder = OpenAILikeEmbedding(
                        model_name=config.vector.model_name,
                        api_base=config.vector.base_url,
                        api_key=config.vector.api_key
        )
        self.classifier = HybridClassifier(self.llm)
        self.projector = SemanticProjector(self.embedder)
        if synthesizer:
            self.synthesizer = synthesizer
        else:
            self.synthesizer = EntitySynthesizer(self.llm)

        # 2. DB Connections
        self.neo4j = Neo4jConnector(uri=config.neo4j.uri, user=config.neo4j.user, password=config.neo4j.password)
        self.qdrant = QdrantClient(url=config.qdrant.url)
        self._init_lookup_collection()

        # 3. Macro-Splitter
        # self.macro_splitter = TokenTextSplitter(
        #     chunk_size=4_000, 
        #     chunk_overlap=250,
        #     separator="\n\n"
        # )
        self.macro_splitter = SemanticSceneSplitter(llm=self.llm, window_size=30_000)

        # 4. === Programs ===
        self._init_programs()

        # 5. Runtime State (Global Registries)
        self.global_entity_registry: Dict[str, str] = {} # Name -> UUID
        self.scene_map: Dict[int, str] = {} # Chunk_Index -> Location_UUID
        
        # 5. Pipeline Options (NEW)
        self.options = options
    
    def _init_programs(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –ø—Ä–æ–≥—Ä–∞–º–º —Å —á–∏—Å—Ç—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ (–±–µ–∑ JSON —Å—Ö–µ–º)."""
        
        # A. ENTITY EXTRACTOR PROMPT [cite: 19]
        # –ú—ã –æ–±—ä—è—Å–Ω—è–µ–º —Å—É—Ç—å —Ç–∏–ø–æ–≤, –Ω–æ —Ñ–æ—Ä–º–∞—Ç –æ—Å—Ç–∞–≤–ª—è–µ—Ç Pydantic
        entity_prompt = PromptTemplate(
            "Analyze the narrative text. Extract persistent 'Canonical Molecules'.\n"
            "Classify them into 6 types based on function:\n\n"
            "1. AGENT: Living beings (Characters, Monsters).\n"
            "2. GROUP: Social structures (Factions, Armies).\n"
            "3. ASSET: Material objects, NOT living beings. Subtypes: 'ARTIFACT' (Unique/Named) vs 'COMMODITY' (Fungible/Resource).\n"
            "4. LOCATION: Physical places or Biomes.\n"
            "5. CONSTRUCT: Skills, Spells, Tech, Phenomena.\n"
            "6. LORE: Secrets, Legends, Information.\n\n"
            "RULES:\n"
            "- Ignore transient items (e.g. 'a cup' unless critical).\n"
            "- Merge synonyms ('The Cat' = 'Cheshire Cat').\n"
            "- Extract 'Fireball' as a CONSTRUCT, not an object.\n\n"
            "TEXT:\n{text}\n"
        )
        self.entity_program = LLMTextCompletionProgram(
            output_cls=EntityBatch,
            llm=self.llm,
            prompt=entity_prompt,
            verbose=True,
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )

        # B. SCENE SEGMENTATION PROMPT
        # –í–º–µ—Å—Ç–æ —Å–ª–æ–∂–Ω–æ–≥–æ Resolver'–∞ –ø—Ä–æ—Å—Ç–æ —Å–ø—Ä–∞—à–∏–≤–∞–µ–º "–ì–¥–µ –º—ã?" –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫—É—Å–∫–∞
        scene_prompt = PromptTemplate(
            "Analyze the text chunk. Identify the PRIMARY physical location where the events occur.\n"
            "If the location changes, identify the dominant one.\n"
            "Provide a brief summary of the scene.\n\n"
            "TEXT:\n{text}\n"
        )
        self.scene_program = LLMTextCompletionProgram(
            output_cls=SceneBatch,
            llm=self.llm,
            prompt=scene_prompt,
            verbose=True,
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )

        event_prompt = PromptTemplate(
            "Analyze the following SCENE from a story.\n"
            "Extract the chain of EVENTS (Beats) in strict chronological order.\n"
            "Granularity: Focus on significant actions and changes in state.\n"
            "- Good: 'Alice sees the rabbit', 'Alice chases the rabbit', 'Alice falls'.\n"
            "- Bad: 'Alice went on an adventure' (Too abstract).\n\n"
            "SCENE TEXT:\n{text}\n"
        )
        self.event_program = LLMTextCompletionProgram(
            output_cls=SceneEventBatch,
            llm=self.llm,
            prompt=event_prompt,
            verbose=True,
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )

    def _find_historic_event(self, query_text: str, threshold: float = 0.85) -> Optional[str]:
        """
        –ò—â–µ—Ç —Å–æ–±—ã—Ç–∏–µ –≤ –ü–†–û–®–õ–û–ú (—É–∂–µ –∑–∞–ø–∏—Å–∞–Ω–Ω–æ–º –≤ Qdrant).
        """
        vec = self.embedder.get_text_embedding(query_text)
        
        # –í–∞–∂–Ω–æ: –∏—â–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ chronicle
        if not self.qdrant.collection_exists("chronicle"):
            return None

        result = self.qdrant.query_points(
            collection_name="chronicle",
            query=vec,
            limit=1
        )
        
        hits = result.points
        if hits and hits[0].score > threshold:
            existing_name = hits[0].payload.get('name', 'Unknown')
            print(f"         üï∞Ô∏è Detected Flashback: '{query_text[:30]}...' -> '{existing_name}' ({hits[0].score:.2f})")
            return hits[0].id
        return None

    def _pass_3_chronicle(self, full_text: str, scene_ranges: List[Tuple[int, int, str, dict]], source_doc: str):
        """
        –ü—Ä–æ—Ö–æ–¥ 3: –•—Ä–æ–Ω–∏–∫–∞ –∏ –°—é–∂–µ—Ç.
        –°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ: (Scene) -> [CONTAINS] -> (Events).
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–ª–µ—à–±–µ–∫–∏, –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç—å –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å–æ–±—ã—Ç–∏–π.
        """
        print(f"   üé¨ Pass 3: Extracting Narrative Chronicle (Hierarchy Mode)...")
        
        global_tick = 0
        last_beat_uuid = None       # –ö—É—Ä—Å–æ—Ä –¥–ª—è —Å–≤—è–∑–∏ —Å–æ–±—ã—Ç–∏–π (Event -> NEXT -> Event)
        last_scene_uuid = None      # –ö—É—Ä—Å–æ—Ä –¥–ª—è —Å–≤—è–∑–∏ —Å—Ü–µ–Ω (Scene -> NEXT -> Scene)
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∑–∞—Ä–∞–Ω–µ–µ –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–º —Å—Ü–µ–Ω–∞–º (–∏–∑ Pass 1)
        for start, end, loc_uuid, context_data in scene_ranges:
            scene_text = full_text[start:end]
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫—É—Å–∫–∏ (–æ–±—ã—á–Ω–æ –º—É—Å–æ—Ä –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏)
            if len(scene_text) < 50: 
                continue
            # –ï—Å–ª–∏ —ç—Ç–æ MEMORY, –º—ã –º–æ–∂–µ–º –ø–æ–º–µ—Ç–∏—Ç—å —Å–æ–∑–¥–∞–≤–∞–µ–º—ã–π –≠–ø–∏–∑–æ–¥ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ñ–ª–∞–≥–æ–º
            is_memory = context_data["type"] in ["MEMORY", "DREAM"]
            
            try:
                # 1. LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –°—Ü–µ–Ω—ã –∏ —Å–ø–∏—Å–æ–∫ –ë–∏—Ç–æ–≤
                prompt_text = f"[SCENE TYPE: {context_data['type']} | SUMMARY: {context_data['label']}]\n{scene_text}"
                response: SceneEventBatch = self.event_program(text=prompt_text)
                
                if not response.events: continue

                # =========================================================
                # LEVEL 2: MACRO (SCENE / EPISODE)
                # =========================================================
                scene_uuid = str(uuid.uuid4())
                scene_start_tick = global_tick + 1
                
                print(f"      üé¨ Processing Scene: '{response.scene_title}' ({len(response.events)} beats)")

                # A. –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –°—Ü–µ–Ω—É (–¥–ª—è –ø–æ–∏—Å–∫–∞ –ê—Ä–æ–∫ –∏ RAG –ø–æ —ç–ø–∏–∑–æ–¥–∞–º)
                # –°–∞–º–º–∞—Ä–∏ —Å—Ü–µ–Ω—ã –ª—É—á—à–µ –ø–µ—Ä–µ–¥–∞–µ—Ç —Å–º—ã—Å–ª –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å—é–∂–µ—Ç–∞, —á–µ–º –º–µ–ª–∫–∏–µ –±–∏—Ç—ã.
                scene_vec_text = f"{response.scene_title}. {response.scene_summary}"
                scene_vec = self.embedder.get_text_embedding(scene_vec_text)
                
                # B. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≠–ø–∏–∑–æ–¥ –≤ Neo4j –∏ –≤—è–∂–µ–º –∫ –õ–æ–∫–∞—Ü–∏–∏
                self.neo4j.upsert_episode(
                    uid=scene_uuid,
                    name=response.scene_title,
                    summary=response.scene_summary,
                    start_tick=scene_start_tick,
                    location_id=loc_uuid
                    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–ª–µ is_dream –≤ –º–µ—Ç–æ–¥ upsert_episode
                )
                
                # –ï—Å–ª–∏ —ç—Ç–æ MEMORY, –ª–∏–Ω–∫—É–µ–º –∫ –ª–æ–∫–∞—Ü–∏–∏ –Ω–µ –∫–∞–∫ HAPPENED_AT, –∞ –∫–∞–∫ RECALLED_AT?
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º HAPPENED_AT, –Ω–æ –≤ summary –±—É–¥–µ—Ç –Ω–∞–ø–∏—Å–∞–Ω–æ "Alice remembered..."

                # C. –°–≤—è–∑—ã–≤–∞–µ–º –°—Ü–µ–Ω—ã —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ (–¶–µ–ø–æ—á–∫–∞ —ç–ø–∏–∑–æ–¥–æ–≤)
                if last_scene_uuid:
                    self.neo4j.link_episode_chain(last_scene_uuid, scene_uuid)

                # D. –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –°—Ü–µ–Ω—É –≤ Qdrant
                self.qdrant.upsert("chronicle", [PointStruct(
                    id=scene_uuid,
                    vector=scene_vec,
                    payload={
                        "name": response.scene_title,
                        "description": response.scene_summary,
                        "type": "episode",      # –¢–∏–ø —É–∑–ª–∞
                        "granularity": "macro", # –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
                        "source": source_doc,
                        "tick": scene_start_tick
                    }
                )])

                # =========================================================
                # LEVEL 1: MICRO (BEATS / EVENTS)
                # =========================================================
                for beat in response.events:
                    
                    # --- –õ–û–ì–ò–ö–ê –ê: –ü–†–û–î–û–õ–ñ–ï–ù–ò–ï (Merge) ---
                    # –ï—Å–ª–∏ LLM –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ —ç—Ç–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
                    if beat.is_continuation and last_beat_uuid:
                        print(f"         üìé Merging continuation...")
                        # –î–æ–ø–∏—Å—ã–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤ Neo4j
                        self.neo4j.driver.execute_query(
                            """
                            MATCH (e:Event {id: $eid})
                            SET e.description = e.description + '\n\n' + $new_desc
                            """,
                            eid=last_beat_uuid, new_desc=beat.description
                        )
                        # (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∏ Qdrant payload, –Ω–æ —ç—Ç–æ –¥–æ—Ä–æ–≥–æ.
                        # –û–±—ã—á–Ω–æ –ø–æ–∏—Å–∫ –Ω–∞—Ö–æ–¥–∏—Ç —Å–æ–±—ã—Ç–∏–µ –∏ –ø–æ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏—è).
                        continue 

                    # --- –õ–û–ì–ò–ö–ê –ë: –§–õ–ï–®–ë–ï–ö (Recollection) ---
                    if beat.is_flashback:
                        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏, –æ —á–µ–º –∏–º–µ–Ω–Ω–æ –≤—Å–ø–æ–º–∏–Ω–∞–µ—Ç –≥–µ—Ä–æ–π
                        historic_id = self._find_historic_event(beat.description)
                        
                        if historic_id:
                            # 1. –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–µ–∞–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ –≤ –ø—Ä–æ—à–ª–æ–º
                            print(f"         üß† Linked Flashback to History: {historic_id}")
                            if last_beat_uuid:
                                self.neo4j.driver.execute_query(
                                    "MATCH (curr:Event {id: $cid}), (old:Event {id: $oid}) MERGE (curr)-[:RECALLS]->(old)",
                                    cid=last_beat_uuid, oid=historic_id
                                )
                        else:
                            # 2. –ï—Å–ª–∏ —ç—Ç–æ "Backstory" (—Å–æ–±—ã—Ç–∏–µ –¥–æ –Ω–∞—á–∞–ª–∞ –∏–≥—Ä—ã) –∏–ª–∏ –ª–æ–∂–Ω–∞—è –ø–∞–º—è—Ç—å
                            mem_uuid = str(uuid.uuid4())
                            print(f"         ‚ú® Created Detached Memory: {beat.name}")
                            
                            # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –≤–Ω–µ –≤—Ä–µ–º–µ–Ω–∏ (tick = -1)
                            self.neo4j.upsert_event(mem_uuid, beat.name, tick_estimate=-1)
                            
                            # –õ–∏–Ω–∫—É–µ–º "–≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ" –∫ —Ç–µ–∫—É—â–µ–º—É –º–æ–º–µ–Ω—Ç—É
                            if last_beat_uuid:
                                self.neo4j.driver.execute_query(
                                    "MATCH (curr:Event {id: $cid}), (mem:Event {id: $mid}) MERGE (curr)-[:RECALLS]->(mem)",
                                    cid=last_beat_uuid, mid=mem_uuid
                                )
                            
                            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å –≤ Qdrant (—á—Ç–æ–±—ã –ø–æ—Ç–æ–º –µ—ë –º–æ–∂–Ω–æ –±—ã–ª–æ –≤—Å–ø–æ–º–Ω–∏—Ç—å)
                            mem_vec = self.embedder.get_text_embedding(beat.description)
                            self.qdrant.upsert("chronicle", [PointStruct(
                                id=mem_uuid, 
                                vector=mem_vec, 
                                payload={
                                    "name": beat.name, 
                                    "description": beat.description, 
                                    "tick": -1, 
                                    "type": "memory",
                                    "granularity": "micro"
                                }
                            )])
                        
                        # –í–∞–∂–Ω–æ: –§–ª–µ—à–±–µ–∫ –Ω–µ —Å–¥–≤–∏–≥–∞–µ—Ç last_beat_uuid –∏ global_tick!
                        continue

                    # --- –õ–û–ì–ò–ö–ê –í: –°–¢–ê–ù–î–ê–†–¢–ù–´–ô –ü–û–¢–û–ö (Standard Beat) ---
                    global_tick += 1
                    evt_uuid = str(uuid.uuid4())
                    
                    # 1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (Hybrid Search: Vector + LLM check)
                    archetype_id = None
                    if self.options.project_events:
                        query_for_classifier = f"{beat.name}. {beat.description}"
                        archetype_id = self.classifier.classify(
                            query_text=query_for_classifier,
                            registry=EVENTS,       # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ–µ—Å—Ç—Ä —Å–æ–±—ã—Ç–∏–π
                            threshold_high=0.88,
                            threshold_low=0.45,
                            top_k=5
                        )

                    # 2. –†–∞—Å—á–µ—Ç –∏–≥—Ä–æ–≤—ã—Ö —Å—Ç–∞—Ç–æ–≤ (—á–µ—Ä–µ–∑ Projector)
                    # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–∞—Å–∫–∏ (Bias), –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –º–æ–ª–µ–∫—É–ª–∞–º
                    vec_text = f"{beat.name}. {beat.description}"
                    embedding = self.embedder.get_text_embedding(vec_text)
                    evt_stats = self.projector.project(embedding)

                    # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Neo4j
                    self.neo4j.upsert_event(
                        evt_uuid, 
                        beat.name, 
                        global_tick, 
                        archetype_id=archetype_id, 
                        semantic_stats=evt_stats
                    )
                    
                    # 4. –°–≤—è–∑–∏ –≥—Ä–∞—Ñ–∞
                    # –ê. –í–∫–ª–∞–¥—ã–≤–∞–µ–º –ë–∏—Ç –≤ –°—Ü–µ–Ω—É (Hierarchy)
                    self.neo4j.link_episode_to_event(scene_uuid, evt_uuid)
                    
                    # –ë. –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è –ë–∏—Ç–æ–≤ (Next)
                    if last_beat_uuid:
                        self.neo4j.driver.execute_query(
                            "MATCH (a:Event {id: $aid}), (b:Event {id: $bid}) MERGE (a)-[:NEXT]->(b)",
                            aid=last_beat_uuid, bid=evt_uuid
                        )
                    
                    # –í. –ü—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç—å (Causality)
                    # –ï—Å–ª–∏ LLM –≤—ã–¥–µ–ª–∏–ª–∞ —è–≤–Ω—É—é –ø—Ä–∏—á–∏–Ω—É (MOTIVATION / ENABLE)
                    if last_beat_uuid and beat.causal_tag and beat.causal_tag != "NONE":
                        self.neo4j.link_causality(last_beat_uuid, evt_uuid, beat.causal_tag)

                    # 5. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant
                    payload = {
                        "name": beat.name,
                        "description": beat.description,
                        "type": "beat",
                        "granularity": "micro",
                        "tick": global_tick,
                        "source": source_doc,
                        "archetype_id": archetype_id,
                        "parent_scene_id": scene_uuid, # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ä–æ–¥–∏—Ç–µ–ª—è
                        "stats": evt_stats
                    }
                    self.qdrant.upsert("chronicle", [PointStruct(id=evt_uuid, vector=embedding, payload=payload)])
                    
                    # –°–¥–≤–∏–≥–∞–µ–º –∫—É—Ä—Å–æ—Ä —Å–æ–±—ã—Ç–∏—è
                    last_beat_uuid = evt_uuid

                # –°–¥–≤–∏–≥–∞–µ–º –∫—É—Ä—Å–æ—Ä —Å—Ü–µ–Ω—ã
                last_scene_uuid = scene_uuid

            except Exception as e:
                logging.error(f"Error in Scene Pass (Chunk {start}-{end}): {e}", exc_info=True)

    def _resolve_or_create_location_id(self, name: str, summary: str) -> str:
        """
        Hybrid Search –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ª–æ–∫–∞—Ü–∏–π.
        """
        # 1. FUZZY MATCH (Neo4j - –ò–º–µ–Ω–∞)
        fuzzy_id = self.neo4j.fuzzy_search_location(name, threshold=0.9)
        if fuzzy_id:
            return fuzzy_id

        # 2. SEMANTIC MATCH (Qdrant - –û–ø–∏—Å–∞–Ω–∏—è)
        vec_text = f"{name}. {summary}"
        query_vector = self.embedder.get_text_embedding(vec_text)
        
        result = self.qdrant.query_points(
            collection_name="skeleton_locations",
            query=query_vector,
            limit=1
        )

        hits = result.points
        
        if hits and hits[0].score > 0.92:
            existing_name = hits[0].payload.get("name", "Unknown")
            print(f"   üß† Qdrant Semantic Match: '{name}' ‚âà '{existing_name}' (Score: {hits[0].score:.2f})")
            return hits[0].id

        # 3. NO MATCH -> Create New
        return str(uuid.uuid4())

    def _init_lookup_collection(self):
        """
        –°–æ–∑–¥–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è –≥—Ä–∞—Ñ–∞ (–ª–æ–∫–∞—Ü–∏–∏-—Å–∫–µ–ª–µ—Ç—ã –∏ —Ö—Ä–æ–Ω–∏–∫–∏).
        """
        # skeleton_locations: –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ "–ì–¥–µ —è?" –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é
        if not self.qdrant.collection_exists("skeleton_locations"):
            self.qdrant.create_collection(
                collection_name="skeleton_locations",
                vectors_config=VectorParams(size=config.v_size, distance=Distance.COSINE),
                shard_number=1 # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
            )
        
        # chronicle: –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π (—Ñ–ª–µ—à–±–µ–∫–∏)
        if not self.qdrant.collection_exists("chronicle"):
             self.qdrant.create_collection(
                collection_name="chronicle",
                vectors_config=VectorParams(size=config.v_size, distance=Distance.COSINE),
                shard_number=1
            )

    def build_world_skeleton(self, full_text: str, source_doc: str) -> Tuple[List[Tuple[int, int, str]], Dict[str, str]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        1. scene_ranges: –°–ø–∏—Å–æ–∫ [(start_char, end_char, location_uuid)]
        2. entity_registry: –°–ª–æ–≤–∞—Ä—å { "canonical_name": "uuid" }
        """
        print(f"üèóÔ∏è  Starting World Architecture for: {source_doc}")
        
        doc = Document(text=full_text, id_=source_doc)
        macro_nodes: List[BaseNode] = self.macro_splitter.get_nodes_from_documents([doc])
        
        # --- STEP 1: SCENE MAPPING ---
        print(f"   üó∫Ô∏è  Pass 1: Mapping Scenes & Locations ({len(macro_nodes)} chunks)...")
        # –í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º full_text –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–µ–∫—Å–æ–≤
        scene_ranges = self._pass_1_scenes(macro_nodes, full_text, source_doc)
        
        # --- STEP 2: GLOBAL ENTITY REGISTRY ---
        print(f"   üß¨ Pass 2: Extracting Canonical Molecules...")
        chunks_text = [n.text for n in macro_nodes] 
        self._pass_2_entities(chunks_text, source_doc)

        # --- STEP 3: CHRONICLE ---
        print(f"   üé¨ Pass 3: Extracting Narrative Chronicle...")
        self._pass_3_chronicle(full_text, scene_ranges, source_doc)

        print("‚úÖ Skeleton Build Complete.")
        return scene_ranges, self.global_entity_registry
    
    def _pass_1_scenes(self, nodes: List[BaseNode], full_text: str, source_doc: str) -> List[Tuple[int, int, str, dict]]:
        """
        –ü—Ä–æ—Ö–æ–¥ 1: –¢–æ–ø–æ–ª–æ–≥–∏—è –∏ –°–∫–µ–ª–µ—Ç.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç SemanticSplitter –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü –∏ —Ç–∏–ø–æ–≤ —Å—Ü–µ–Ω.
        """
        scene_ranges = []
        text_cursor = 0 
        prev_loc_uuid = None
        
        print(f"   üïµÔ∏è Pass 1: Semantic Scene Survey...")

        for i, node in enumerate(nodes):
            # 1. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç –°–ø–ª–∏—Ç—Ç–µ—Ä–∞
            # –°–ø–ª–∏—Ç—Ç–µ—Ä —É–∂–µ –Ω–∞—Ä–µ–∑–∞–ª —Ç–µ–∫—Å—Ç –ø–æ —Å–º—ã—Å–ª—É, –ø–æ—ç—Ç–æ–º—É node_text ‚Äî —ç—Ç–æ —Ü–µ–ª—å–Ω–∞—è —Å—Ü–µ–Ω–∞.
            node_text = node.get_content(metadata_mode=MetadataMode.NONE)
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –¥–æ–±–∞–≤–∏–ª–∏ –≤ RobustSemanticSplitter
            scene_type = node.metadata.get("scene_type", "PHYSICAL")   # PHYSICAL / MEMORY / DREAM / DOCUMENT
            context_label = node.metadata.get("context_label", "")     # "Alice enters the forest"
            
            # === –†–ê–°–ß–ï–¢ –ö–û–û–†–î–ò–ù–ê–¢ ===
            # (–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –Ω–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞)
            start_idx = node.metadata.get('start_char_idx')
            if start_idx is None:
                start_idx = full_text.find(node_text, text_cursor)
                if start_idx == -1:
                    start_idx = text_cursor
                end_idx = start_idx + len(node_text)
            text_cursor = end_idx
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –æ–±—ä–µ–∫—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–ª—å—à–µ
            context_data = {
                "type": scene_type,
                "label": context_label
            }

            try:
                # === –õ–û–ì–ò–ö–ê –ê: –ú–ï–ù–¢–ê–õ–¨–ù–´–ï –°–¶–ï–ù–´ (MEMORY / DREAM) ===
                if scene_type in ["MEMORY", "DREAM", "THOUGHT"]:
                    print(f"      üß† Detected {scene_type}: '{context_label}'")
                    
                    # –ï—Å–ª–∏ —ç—Ç–æ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ, —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –ª–æ–∫–∞—Ü–∏—è –ù–ï –º–µ–Ω—è–µ—Ç—Å—è.
                    # –ú—ã –Ω–∞—Å–ª–µ–¥—É–µ–º –ª–æ–∫–∞—Ü–∏—é, –≥–¥–µ –≥–µ—Ä–æ–π —Å—Ç–æ—è–ª –¥–æ —ç—Ç–æ–≥–æ.
                    current_loc_uuid = prev_loc_uuid
                    
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–∞–º–æ–µ –Ω–∞—á–∞–ª–æ –∫–Ω–∏–≥–∏ –∏ prev_loc_uuid –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞–µ–º "Limbo/Unknown"
                    if not current_loc_uuid:
                        current_loc_uuid = self._resolve_or_create_location_stub("Unknown Void", "Abstract space", source_doc)

                    # –ú—ã –ù–ï –≤—ã–∑—ã–≤–∞–µ–º scene_program (—ç–∫–æ–Ω–æ–º–∏–º —Ç–æ–∫–µ–Ω—ã), —Ç–∞–∫ –∫–∞–∫ –ª–æ–∫–∞—Ü–∏—è –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å.
                    # –ù–æ –º—ã –º–æ–∂–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–∞–º–º–∞—Ä–∏ –≤ –¥–æ—Å—å–µ –ª–æ–∫–∞—Ü–∏–∏ –∫–∞–∫ "–º—ã—Å–ª—å, –ø–æ—Å–µ—Ç–∏–≤—à–∞—è –∑–¥–µ—Å—å".
                    if hasattr(self, 'synthesizer') and current_loc_uuid:
                         self.synthesizer.collect_location_observation(
                            current_loc_uuid, 
                            f"[ATMOSPHERE/THOUGHT] In this place, the character recalled: {context_label}",
                            "Current Location"
                        )

                # === –õ–û–ì–ò–ö–ê –ë: –§–ò–ó–ò–ß–ï–°–ö–ò–ï –°–¶–ï–ù–´ (PHYSICAL) ===
                else:
                    # –≠—Ç–æ —Ä–µ–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ. –°–ø—Ä–∞—à–∏–≤–∞–µ–º LLM: "–ß—Ç–æ —ç—Ç–æ –∑–∞ –º–µ—Å—Ç–æ?"
                    
                    # –í–ø—Ä—ã—Å–∫–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã LLM –∑–Ω–∞–ª–∞, –æ —á–µ–º —Ä–µ—á—å
                    augmented_text = f"[SCENE CONTEXT: {context_label}]\n{node_text}"
                    
                    response: SceneBatch = self.scene_program(text=augmented_text)
                    
                    if not response.scenes:
                        # –ï—Å–ª–∏ LLM –Ω–µ –ø–æ–Ω—è–ª–∞, –≥–¥–µ –º—ã ‚Äî –æ—Å—Ç–∞–µ–º—Å—è –Ω–∞ –º–µ—Å—Ç–µ
                        current_loc_uuid = prev_loc_uuid
                    else:
                        scene_data = response.scenes[0]
                        raw_name = scene_data.location_name.strip()
                        
                        # –§–∏–ª—å—Ç—Ä: –ï—Å–ª–∏ –∏–º—è —Å–ª–∏—à–∫–æ–º –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                        if len(raw_name) < 3 or raw_name.lower() in ["unknown", "none", "location"]:
                            current_loc_uuid = prev_loc_uuid
                        else:
                            # 1. –†–µ–∑–æ–ª–≤–∏–Ω–≥ ID (Fuzzy/Semantic)
                            current_loc_uuid = self._resolve_or_create_location_stub(
                                raw_name, scene_data.summary, source_doc
                            )
                            
                            # 2. –î–æ—Å—å–µ –õ–æ–∫–∞—Ü–∏–∏
                            if hasattr(self, 'synthesizer'):
                                self.synthesizer.collect_location_observation(
                                    current_loc_uuid, 
                                    scene_data.summary, 
                                    raw_name
                                )

                            # 3. –¢–æ–ø–æ–ª–æ–≥–∏—è (–°–≤—è–∑—å —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π)
                            # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ PHYSICAL -> PHYSICAL –ø–µ—Ä–µ—Ö–æ–¥
                            if prev_loc_uuid and prev_loc_uuid != current_loc_uuid:
                                self.neo4j.link_locations(prev_loc_uuid, current_loc_uuid, "TRANSITION")
                                print(f"      üîó Path: ... -> {raw_name}")

                # === –§–ò–ù–ê–õ–ò–ó–ê–¶–ò–Ø ===
                if current_loc_uuid:
                    scene_ranges.append((start_idx, end_idx, current_loc_uuid, context_data)) # <--- 4 —ç–ª–µ–º–µ–Ω—Ç–∞!
                    prev_loc_uuid = current_loc_uuid
                else:
                    # Fallback –Ω–∞ —Å–ª—É—á–∞–π –ø–µ—Ä–≤–æ–π —Å—Ü–µ–Ω—ã –±–µ–∑ –ª–æ–∫–∞—Ü–∏–∏
                    pass 

            except Exception as e:
                logging.error(f"Error in Scene Pass chunk {i}: {e}", exc_info=True)
        
        return scene_ranges

    def _pass_2_entities(self, chunks: List[str], source_doc: str):
        print(f"   üß¨ Pass 2: Extracting Canonical Molecules...")
        for i, chunk in enumerate(chunks):
            try:
                response: EntityBatch = self.entity_program(text=chunk)
                
                for entity in response.entities:
                    # === –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê ===
                    if entity.category == "LOCATION":
                        # –ù–µ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–∞–∫ –º–æ–ª–µ–∫—É–ª—É!
                        # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∏—â–µ–º/—Å–æ–∑–¥–∞–µ–º Stub –∏ –ø–∏—à–µ–º –≤ –î–æ—Å—å–µ.
                        
                        # 1. –†–µ–∑–æ–ª–≤–∏–º ID (Fuzzy/Semantic Search)
                        loc_id = self._resolve_or_create_location_stub(
                            entity.name, entity.description, source_doc
                        )
                        
                        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –≤ –°–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä
                        self.synthesizer.collect_location_observation(
                            loc_id, 
                            entity.description, 
                            entity.name
                        )
                        print(f"      üè∞ Collected Location Note: {entity.name}")
                        
                    else:
                        # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ (Agent, Asset, Lore) ‚Äî —ç—Ç–æ –º–æ–ª–µ–∫—É–ª—ã
                        self._register_molecule(entity, source_doc)
                    
            except Exception as e:
                logging.error(f"Error in Entity Pass chunk {i}: {e}")

    def _register_molecule(self, entity: DetectedEntity, source_doc: str):
        """
        –£–º–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —Å—É—â–Ω–æ—Å—Ç—å –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º —Ä–µ–µ—Å—Ç—Ä–µ.
        –ï—Å–ª–∏ –Ω–µ—Ç -> —Å–æ–∑–¥–∞–µ—Ç, —Å—á–∏—Ç–∞–µ—Ç —Å—Ç–∞—Ç—ã, –ø–∏—à–µ—Ç –≤ –ë–î.
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ –¥–ª—è –∫–ª—é—á–∞ —Ä–µ–µ—Å—Ç—Ä–∞
        reg_key = entity.name.lower().strip()
        
        if reg_key in self.global_entity_registry:
            # –£–∂–µ –∑–Ω–∞–µ–º —Ç–∞–∫—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Alice" –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å –≤–æ 2-–º —á–∞–Ω–∫–µ –ø–æ—Å–ª–µ 1-–≥–æ)
            return

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π ID
        mol_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, reg_key))
        self.global_entity_registry[reg_key] = mol_uuid

        print(f"      üß¨ New Molecule: [{entity.category}] {entity.name} ({entity.subtype if entity.subtype else ''})")
        
        # === 1. Calculate Stats (Projection) ===
        # –ó–¥–µ—Å—å –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–∏—Å–∞–Ω–∏—è, —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –∏–≥—Ä–æ–≤—ã–µ —Å—Ç–∞—Ç—ã
        vec_text = f"{entity.name}. {entity.description} Type: {entity.category}"
        embedding = self.embedder.get_text_embedding(vec_text)
        game_stats = self.projector.project(embedding)

        # === 2. Special Logic per Type ===
        
        # –ü—Ä–∏–º–µ—Ä: –ï—Å–ª–∏ —ç—Ç–æ COMMODITY (–ó–æ–ª–æ—Ç–æ), —Ñ–æ—Ä—Å–∏—Ä—É–µ–º fungibility=1.0
        if entity.subtype == AssetSubtype.COMMODITY:
            # TODO [cite: 33, 34] Force fungibility for commodities
            # game_stats['mat_fungibility'] = 1.0 (–µ—Å–ª–∏ –ø—Ä–æ–µ–∫—Ç–æ—Ä –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å)
            pass

        # === 3. Upsert to DB ===
        # Qdrant
        payload = {
            "name": entity.name,
            "type": entity.category.value,
            "subtype": entity.subtype.value if entity.subtype else None,
            "description": entity.description,
            "source": source_doc,
            "stats": game_stats
        }
        
        self.qdrant.upsert(
            "molecules",
            [PointStruct(id=mol_uuid, vector=embedding, payload=payload)]
        )
        
        # Neo4j
        # –ó–¥–µ—Å—å —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π —É–∑–µ–ª. –°–≤—è–∑–∏ –¥–æ–±–∞–≤—è—Ç—Å—è –Ω–∞ Micro-Pass –∏–ª–∏ Event-Pass.
        self.neo4j.upsert_molecule(
            mol_uuid, 
            entity.name, 
            entity.category.value,
            semantic_stats=game_stats
        )

        if hasattr(self, 'synthesizer'):
            self.synthesizer.collect(
                uid=mol_uuid,
                observation=f"[MACRO-CONTEXT]: {entity.description}", # –ü–æ–º–µ—á–∞–µ–º, —á—Ç–æ —ç—Ç–æ –º–∞–∫—Ä–æ
                metadata={
                    "name": entity.name,
                    "category": entity.category.value,
                    "subtype": entity.subtype.value if entity.subtype else None,
                    "source_doc": source_doc
                }
            )

    def _resolve_or_create_location_stub(self, name: str, summary: str, source_doc: str) -> str:
        """
        –ò—â–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ª–æ–∫–∞—Ü–∏—é –ø–æ –≤—Å–µ–π –±–∞–∑–µ. –ï—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç ‚Äî —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é.
        """
        # --- –≠–¢–ê–ü 1: FUZZY SEARCH (Neo4j) ---
        # –ò—â–µ–º –æ–ø–µ—á–∞—Ç–∫–∏ –∏–ª–∏ –≤–∞—Ä–∏–∞—Ü–∏–∏ –∏–º–µ–Ω ("Dark Forest" vs "The Dark Forest")
        fuzzy_id = self.neo4j.fuzzy_search_location(name, threshold=0.9) #
        if fuzzy_id:
            return fuzzy_id

        # --- –≠–¢–ê–ü 2: SEMANTIC SEARCH (Qdrant) ---
        # –ò—â–µ–º –ø–æ —Å–º—ã—Å–ª—É –æ–ø–∏—Å–∞–Ω–∏—è. –ü–æ–ª–µ–∑–Ω–æ, –µ—Å–ª–∏ –∏–º—è –¥—Ä—É–≥–æ–µ, –Ω–æ —Å—É—Ç—å —Ç–∞ –∂–µ.
        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º "–ò–º—è + –û–ø–∏—Å–∞–Ω–∏–µ"
        vec_text = f"{name}. {summary}"
        query_vector = self.embedder.get_text_embedding(vec_text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        if self.qdrant.collection_exists("skeleton_locations"):
            result = self.qdrant.query_points(
                collection_name="skeleton_locations",
                query=query_vector,
                limit=1
            )
            hits = result.points
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ–µ (Score > 0.93 - –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
            if hits and hits[0].score > 0.93:
                existing_name = hits[0].payload.get("name", "Unknown")
                print(f"      üß† Qdrant Semantic Match: '{name}' ‚âà '{existing_name}' (Score: {hits[0].score:.2f})")
                return hits[0].id

        # --- –≠–¢–ê–ü 3: –°–û–ó–î–ê–ù–ò–ï –ù–û–í–û–ô ---
        # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é "Stub" –ª–æ–∫–∞—Ü–∏—é
        new_id = str(uuid.uuid4())
        
        # A. –ü–∏—à–µ–º –≤ Neo4j (—á—Ç–æ–±—ã fuzzy search –Ω–∞—Ö–æ–¥–∏–ª –µ—ë –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —á–∞–Ω–∫–æ–≤)
        self.neo4j.upsert_location(
            loc_id=new_id, 
            name=name, 
            summary=summary, 
            source_doc=source_doc,
            semantic_stats=None # –°—Ç–∞—Ç—ã –ø–æ—Å—á–∏—Ç–∞–µ–º –ø–æ–∑–∂–µ –∏–ª–∏ —Ç—É—Ç –∂–µ, –µ—Å–ª–∏ —Ö–æ—Ç–∏–º
        ) #
        
        # B. –ü–∏—à–µ–º –≤ Qdrant (—á—Ç–æ–±—ã semantic search –Ω–∞—Ö–æ–¥–∏–ª –µ—ë –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —á–∞–Ω–∫–æ–≤)
        # –ù–∞–º –Ω—É–∂–Ω—ã —Ö–æ—Ç—å –∫–∞–∫–∏–µ-—Ç–æ —Å—Ç–∞—Ç—ã –¥–ª—è payload, —Å–¥–µ–ª–∞–µ–º –ø—Ä–æ–µ–∫—Ü–∏—é —Å–µ–π—á–∞—Å
        loc_stats = self.projector.project(query_vector)
        
        self.qdrant.upsert(
            "skeleton_locations",
            [PointStruct(
                id=new_id,
                vector=query_vector,
                payload={
                    "name": name, 
                    "summary": summary,
                    "source": source_doc,
                    "stats": loc_stats
                }
            )]
        )
        
        print(f"      ‚ú® Created New Location: '{name}'")
        return new_id

    def _process_locations(
            self, locations: List[GraphLocation], 
            connections: List[LocationConnection], 
            source_doc: str) -> Dict[str, str]:
        slug_to_uuid = {}
        
        # --- NODES (–õ–û–ö–ê–¶–ò–ò) ---
        for loc in locations:
            # 1. –†–µ–∑–æ–ª–≤–∏–Ω–≥ ID
            real_uuid = self._resolve_or_create_location_id(loc.name, loc.summary)
            slug_to_uuid[loc.suggested_id] = real_uuid
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞
            vec_text = f"{loc.name}. {loc.summary}"
            embedding = self.embedder.get_text_embedding(vec_text)
            
            # 2. === PROJECTION: TOPOLOGY & STATS ===
            template_id = None
            loc_stats = None
            
            if self.options.project_topology:
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Ä–µ–µ—Å—Ç—Ä
                query_text = f"{loc.type}. {loc.summary}"
                found_templates = TOPOLOGIES.classify(query_text, threshold=0.6, top_k=1)
                
                if found_templates:
                    template_obj = found_templates[0][0] # –°–∞–º –æ–±—ä–µ–∫—Ç TopologyTemplate
                    template_id = template_obj.id
                    
                    # === –õ–û–ì–ò–ö–ê –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ê ===
                    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —à–∞–±–ª–æ–Ω, –±–µ—Ä–µ–º –µ–≥–æ "–ò–¥–µ–∞–ª—å–Ω—ã–π –í–µ–∫—Ç–æ—Ä" (query_vector)
                    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ "–¢—é—Ä—å–º–∞" –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç –æ—â—É—â–∞—Ç—å—Å—è –∫–∞–∫ "–¢—é—Ä—å–º–∞"
                    loc_stats = template_obj.query_vector.model_dump()
                    
                    print(f"   üó∫Ô∏è Mapped Location '{loc.name}' -> Template '{template_id}' (Using Static Stats)")
            
            # –ï—Å–ª–∏ —à–∞–±–ª–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω (–∏–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∞ –ø—Ä–æ–µ–∫—Ü–∏—è), –≤—ã—á–∏—Å–ª—è–µ–º "–í–∞–π–±" –∏–∑ —Ç–µ–∫—Å—Ç–∞
            if not loc_stats:
                # SemanticProjector –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                loc_stats = self.projector.project(embedding)
                print(f"   üé® Calculated Dynamic Stats for '{loc.name}'")

            # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ (DB)
            if template_id:
                self.neo4j.upsert_location_projection(real_uuid, template_id)

            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Neo4j (–ø–µ—Ä–µ–¥–∞–µ–º stats)
            self.neo4j.upsert_location(
                real_uuid, 
                loc.name, 
                loc.summary, 
                source_doc, 
                template_id=template_id,
                semantic_stats=loc_stats # <--- –í–∞–∂–Ω–æ
            )
            
            # 5. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant
            self.qdrant.upsert(
                "skeleton_locations",
                [PointStruct(
                    id=real_uuid,
                    vector=embedding,
                    payload={
                        "name": loc.name, 
                        "slug": loc.suggested_id, 
                        "source": source_doc,
                        "template_id": template_id,
                        "stats": loc_stats # <--- –í–∞–∂–Ω–æ
                    }
                )]
            )
            
        # --- EDGES (–°–í–Ø–ó–ò) ---
        for conn in connections:
            from_id = slug_to_uuid.get(conn.from_slug)
            to_id = slug_to_uuid.get(conn.to_slug)
            if from_id and to_id:
                # TODO: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä—ë–±–µ—Ä (–Ω–æ –º–± —É–∂–µ –µ—Å—Ç—å)
                self.neo4j.link_locations(from_id, to_id, conn.type)
                
        return slug_to_uuid
    
    def _process_chronology_stream(
            self, events: List[GraphEvent], 
            causal_links: List[CausalLink], 
            slug_map: Dict[str, str], 
            start_tick: int, 
            prev_chunk_last_event_id: str, 
            source_doc: str
        ):
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
        sorted_events = sorted(events, key=lambda x: x.order_index)
        local_index_map = {} 
        current_timeline_cursor = prev_chunk_last_event_id
        
        for i, evt in enumerate(sorted_events):
            
            # --- –õ–û–ì–ò–ö–ê 1: –ü–†–û–î–û–õ–ñ–ï–ù–ò–ï ---
            if evt.is_continuation and current_timeline_cursor:
                print(f"   üìé Merging continuation into {current_timeline_cursor}...")
                self.neo4j.driver.execute_query(
                    """
                    MATCH (e:Event {id: $eid})
                    SET e.description = e.description + '\n\n[Continuation]: ' + $new_desc
                    """,
                    eid=current_timeline_cursor, 
                    new_desc=evt.description
                )
                # –ú—ã –æ–±–Ω–æ–≤–ª—è–µ–º payload —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–±—ã—Ç–∏—è –≤ Qdrant? 
                # –ü–æ–∫–∞ –ø—Ä–æ–ø—É—Å—Ç–∏–º, —Å—á–∏—Ç–∞—è –ø–µ—Ä–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã–º.
                local_index_map[evt.order_index] = current_timeline_cursor
                continue 

            # --- –õ–û–ì–ò–ö–ê 2: –í–û–°–ü–û–ú–ò–ù–ê–ù–ò–ï ---
            if evt.is_recollection:
                historic_id = self._find_historic_event(evt.name)
                if historic_id:
                    print(f"   üß† Linking Flashback: Current -> {historic_id}")
                    if current_timeline_cursor:
                         self.neo4j.driver.execute_query(
                            """
                            MATCH (curr:Event {id: $cid}), (old:Event {id: $oid}) 
                            MERGE (curr)-[:RECALLS]->(old)
                            """,
                            cid=current_timeline_cursor, oid=historic_id
                        )
                    continue 
                else:
                    # –°–æ–∑–¥–∞–µ–º "Detached Memory"
                    print(f"   ‚ú® Creating new Memory Node (detached): {evt.name}")
                    memory_uuid = str(uuid.uuid4())
                    
                    self.neo4j.upsert_event(memory_uuid, evt.name, -1)
                    if current_timeline_cursor:
                        self.neo4j.driver.execute_query(
                            "MATCH (curr:Event {id: $cid}), (mem:Event {id: $mid}) MERGE (curr)-[:RECALLS]->(mem)",
                            cid=current_timeline_cursor, mid=memory_uuid
                        )
                    
                    vec_text = f"{evt.name}. {evt.description}"
                    embedding = self.embedder.get_text_embedding(vec_text)
                    evt_stats = self.projector.project(embedding)
                    
                    self._index_event_vector(
                        memory_uuid, evt.name, evt.description, -1, 
                        embedding, evt_stats, source_doc=source_doc # <--- Source
                    )
                    continue

            # --- STANDARD FLOW ---
            evt_uuid = str(uuid.uuid4())
            local_index_map[evt.order_index] = evt_uuid
            absolute_tick = start_tick + i + 1
            
           # === PROJECTION: EVENTS ===
            archetype_id = None
            evt_stats = None # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º None
            
            vec_text = f"{evt.name}. {evt.description}"
            embedding = self.embedder.get_text_embedding(vec_text)

            if self.options.project_events:
                # –ò—â–µ–º –∞—Ä—Ö–µ—Ç–∏–ø —Å–æ–±—ã—Ç–∏—è ("Ambush", "Negotiation")
                found = EVENTS.classify(f"{evt.name}. {evt.description}", threshold=0.2, top_k=1)
                
                if found:
                    archetype_obj = found[0][0]
                    archetype_id = archetype_obj.id
                    
                    # === –õ–û–ì–ò–ö–ê –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ê ===
                    # –ï—Å–ª–∏ —Å–æ–±—ã—Ç–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –∫–∞–∫ "–ë–∏—Ç–≤–∞", –±–µ—Ä–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ë–∏—Ç–≤—ã.
                    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Å—Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ —ç—Ç–æ –∫–∞–∫ –Ω–∞ –±–æ–µ–≤—É—é —Å—Ü–µ–Ω—É.
                    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–æ–ª–µ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è 'vector' –∏–ª–∏ 'step_vector'
                    if hasattr(archetype_obj, 'vector'):
                        evt_stats = archetype_obj.vector.model_dump()
                    elif hasattr(archetype_obj, 'step_vector'): # –ù–∞ —Å–ª—É—á–∞–π –¥—Ä—É–≥–æ–π —Å—Ö–µ–º—ã
                        evt_stats = archetype_obj.step_vector.model_dump()
                        
                    print(f"   ‚öîÔ∏è  Event Projection: '{evt.name}' -> {archetype_id} (Using Static Stats)")

            # –ï—Å–ª–∏ –∞—Ä—Ö–µ—Ç–∏–ø –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ —É –Ω–µ–≥–æ –Ω–µ—Ç –≤–µ–∫—Ç–æ—Ä–∞ ‚Äî —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ–µ–∫—Ü–∏—é —Å–∞–º–∏
            if not evt_stats:
                evt_stats = self.projector.project(embedding)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Neo4j
            self.neo4j.upsert_event(
                evt_uuid, 
                evt.name, 
                absolute_tick, 
                archetype_id=archetype_id,
                semantic_stats=evt_stats # <--- –ü–µ—Ä–µ–¥–∞–µ–º
            )

            # –°–≤—è–∑—å —Å –õ–æ–∫–∞—Ü–∏–µ–π
            if evt.location_slug and evt.location_slug in slug_map:
                self.neo4j.link_event_to_location(evt_uuid, slug_map[evt.location_slug])
            
            # –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å
            if current_timeline_cursor:
                self.neo4j.driver.execute_query(
                    "MATCH (a:Event {id: $aid}), (b:Event {id: $bid}) MERGE (a)-[:NEXT]->(b)",
                    aid=current_timeline_cursor, bid=evt_uuid
                )
            
            # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant
            self._index_event_vector(
                evt_uuid, 
                evt.name, 
                evt.description, 
                absolute_tick, 
                embedding,  
                evt_stats,
                source_doc=source_doc, # <--- Source
                archetype_id=archetype_id
            )

            current_timeline_cursor = evt_uuid

        # --- CAUSALITY ---
        for link in causal_links:
            cause = local_index_map.get(link.cause_event_index)
            effect = local_index_map.get(link.effect_event_index)
            if cause and effect:
                self.neo4j.link_causality(cause, effect, link.reason)
                
        return current_timeline_cursor

    # def _classify_event(self, evt) -> Optional[str]:
    #     """–•–µ–ª–ø–µ—Ä –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ —Ä–µ–µ—Å—Ç—Ä."""
    #     query = f"{evt.name}. {evt.description}"
    #     found = EVENTS.classify(query, threshold=0.2, top_k=1)
    #     if found:
    #         print(f"   ‚öîÔ∏è  Event Projection: '{evt.name}' -> {found[0][0].id}")
    #         return found[0][0].id
    #     return None

    def _index_event_vector(
            self, uuid_str: str, 
            name: str, 
            description: str, 
            tick: int, 
            embedding: Embedding, 
            stats: Dict[str, float], 
            source_doc: str, 
            archetype_id: Optional[str] = None
        ):
        """–•–µ–ª–ø–µ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ Qdrant (Chronicle)."""
        payload = {
            "name": name, 
            "tick": tick, 
            "source": source_doc, # <--- Added
            "description": description,
            "archetype_id": archetype_id,
            "stats": stats
        }
        
        self.qdrant.upsert(
            "chronicle",
            [PointStruct(
                id=uuid_str,
                vector=embedding,
                payload=payload
            )]
        )
    