# src/infrastructure/smart_client.py
import os
import json
import hashlib
import time
import logging
from typing import Any, Dict, Optional, Type, TypeVar, Union
from pydantic import BaseModel

from openai import OpenAI, APIConnectionError, RateLimitError
from openai.types.chat import ChatCompletion
from src.debug.telemetry import telemetry, EventType # –ü–æ–¥–∫–ª—é—á–∞–µ–º –Ω–∞—à—É —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é

T = TypeVar("T", bound=BaseModel)

class SmartOpenAI:
    def __init__(
        self, 
        api_key: str, 
        base_url: str, 
        cache_dir: str = "cache/openai_global",
        max_retries: int = 5
    ):
        self._client = OpenAI(api_key=api_key, base_url=base_url)
        self._cache_dir = cache_dir
        self._max_retries = max_retries
        
        if not os.path.exists(self._cache_dir):
            os.makedirs(self._cache_dir)

    def _get_cache_key(self, messages: list, model: str, extra: dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö—ç—à –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤—Å—ë, —á—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        payload = {
            "messages": messages,
            "model": model,
            **extra
        }
        raw = json.dumps(payload, sort_keys=True)
        return hashlib.md5(raw.encode('utf-8')).hexdigest()

    def _load_cache(self, key: str) -> Optional[Dict]:
        path = os.path.join(self._cache_dir, f"{key}.json")
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        return None

    def _save_cache(self, key: str, data: Any):
        path = os.path.join(self._cache_dir, f"{key}.json")
        with open(path, "w", encoding="utf-8") as f:
            # –ï—Å–ª–∏ —ç—Ç–æ Pydantic –º–æ–¥–µ–ª—å –∏–ª–∏ OpenAI –æ–±—ä–µ–∫—Ç, —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º
            if hasattr(data, "model_dump"):
                json_data = data.model_dump(mode='json')
            elif hasattr(data, "to_dict"): # OpenAI v1 objects
                json_data = data.to_dict()
            else:
                json_data = data
                
            json.dump(json_data, f, ensure_ascii=False, indent=2)

    def chat_completion(
        self,
        messages: list,
        model: str,
        response_format: Optional[Type[T]] = None,
        **kwargs
    ) -> Union[ChatCompletion, T]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥:
        - –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω response_format (Pydantic –∫–ª–∞—Å—Å) -> –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ (Structured Output).
        - –ï—Å–ª–∏ –Ω–µ—Ç -> –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ã—á–Ω—ã–π ChatCompletion.
        - –í–∫–ª—é—á–∞–µ—Ç –ö—ç—à, –†–µ—Ç—Ä–∞–∏ –∏ –¢–µ–ª–µ–º–µ—Ç—Ä–∏—é.
        """
        
        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª—é—á–∞ –∫—ç—à–∞
        # –ï—Å–ª–∏ response_format —ç—Ç–æ –∫–ª–∞—Å—Å, –±–µ—Ä–µ–º –µ–≥–æ schema_json –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        schema_sig = response_format.model_json_schema() if response_format else "raw_text"
        cache_key = self._get_cache_key(messages, model, {"schema": schema_sig, **kwargs})

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cached_data = self._load_cache(cache_key)
        if cached_data:
            # üì° TELEMETRY: Cache Hit
            telemetry.emit(EventType.STEP_INFO, "SmartClient Cache Hit", {"key": cache_key})
            
            if response_format:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Pydantic –æ–±—ä–µ–∫—Ç –∏–∑ JSON
                return response_format.model_validate(cached_data)
            else:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º OpenAI –æ–±—ä–µ–∫—Ç (–Ω–µ–º–Ω–æ–≥–æ —É–ø—Ä–æ—â–µ–Ω–Ω–æ, –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤–∞—Ä—è)
                # –î–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å openai.types.chat.ChatCompletion.model_validate
                return ChatCompletion.model_validate(cached_data)

        # 3. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å —Ä–µ—Ç—Ä–∞—è–º–∏
        last_error = None
        for attempt in range(self._max_retries):
            try:
                # üì° TELEMETRY: Request
                telemetry.emit(EventType.LLM_REQ, f"OpenAI Call (Att: {attempt+1})", {
                    "model": model,
                    "messages": messages[-1] if messages else []
                })

                start_time = time.time()
                
                if response_format:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π .parse() –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å OpenAI
                    completion = self._client.chat.completions.parse(
                        model=model,
                        messages=messages,
                        response_format=response_format,
                        **kwargs
                    )
                    if hasattr(completion, 'choices'):
                        result = completion.choices[0].message.parsed
                    elif hasattr(completion, 'output_parsed'):
                        result = completion.output_parsed
                    else:
                        result = completion
                else:
                    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
                    completion = self._client.chat.completions.create(
                        model=model,
                        messages=messages,
                        **kwargs
                    )
                    result = completion

                duration = time.time() - start_time

                # üì° TELEMETRY: Success
                telemetry.emit(EventType.LLM_RES, f"OpenAI Success ({duration:.2f}s)", {
                    "tokens": completion.usage.total_tokens if completion.usage else 0
                })

                # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
                self._save_cache(cache_key, result)
                
                return result

            except (RateLimitError, APIConnectionError) as e:
                last_error = e
                wait_time = 2 ** attempt # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                telemetry.emit(EventType.ERROR, f"Retryable Error: {e}", {"wait": wait_time})
                time.sleep(wait_time)
            except Exception as e:
                # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 400 Bad Request) –Ω–µ —Ä–µ—Ç—Ä–∞–∏–º
                telemetry.emit(EventType.ERROR, f"Critical OpenAI Error", {"error": str(e)})
                raise e

        raise last_error
    